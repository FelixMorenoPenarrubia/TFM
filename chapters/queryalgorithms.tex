\todowritehp{Finish writing Chapter 3}

In this chapter, we study constructive separations in the query algorithm setting.
In particular, we study the problem $\MAJ$ of distinguishing determining whether
a binary string $x \in \{0, 1\}^n$ has at least $(\frac{1}{2}+\eps)n$ ones or
at most $(\frac{1}{2}-\eps)n$ ones, under the promise that it is one of the two
cases. 

We will prove the $\Omega(1/\eps^2)$ lower bound in the number of queries
for $\MAJ$, show a randomized constructive separation against algorithms
using fewer queries, and prove and discuss \cref{thm:csrefuterqa}, the result
from \cite{ConstructiveSeparations} showing that breakthrough lower
bounds follow from sufficiently constructive separations of $\MAJ$.  


\section{Lower Bounds Against Query Algorithms for $\MAJ$}
\label{sec:lbqa}

The obvious algorithm for $\MAJ$ consists in randomly sampling values and answering 
according to the majority of the values we have obtained. According to well-studied
bounds on the binomial distribution, $\Theta(1/\eps^2)$ samples
are necessary and sufficient to make the probability of error smaller than any 
fixed constant following this strategy.

In this section, we will prove the very intuitive result that this is essentially
the best that can be done, that is, that there is no query algorithm making $o(1/\eps^2)$
queries that gives the right answer with small probability of error. This was 
proved in \cite{Canetti95} in the more general context of \emph{samplers}: algorithms
that compute the average of a real-valued function $f \colon \{0, 1\}^n \to [0, 1]$
by sampling some of the inputs. 

The result that we will prove here is stronger than the one stated in \cite{Canetti95}:
we will prove not just that such query algorithms require $\Omega(1/\eps^2)$ in the 
worst case in order to have a high success probability \emph{for all} inputs; instead,
we will prove that $\Omega(1/\eps^2)$ queries are required for \emph{random} inputs,
where the probability of success is over the randomness of both the input and the algorithm.
This will allow us to obtain a constructive separation.
%However, the proof argument 
%we use is similar to the one used in \cite{Canetti95}.

\subsection{Proof of Lower Bounds for Random Inputs}



\begin{definition}
A probability distribution $\calD$ on $\{0, 1\}^n$ has \emph{homogeneous
symmetry} if for every $y \in \{0, 1\}^*$ and every pair of subsets $S, S' \subset [n]$
with $|S| = |S'| = |y|$, 
$\Probsub{x \samplefrom \calD}{x[S] = y} = \Probsub{x \samplefrom \calD}{x[S'] = y}$.
That is, the probability $\Probsub{x \samplefrom \calD}{x[S] = y}$ does not depend
on the set $S$. Denote by $F_{\calD}(y)$ the probability
$\Probsub{x \samplefrom \D}{x[S] = y}$ for any $S$ with $|S| = |y|$.

A probability distribution $\calD$ on $\{0, 1\}^n$ has \emph{complementary symmetry} if for every
$y in \{0, 1\}^n$, $\Probsub{x \samplefrom \D}{x = y} = \Probsub{x \samplefrom \D}{x = \complementary{y}}$,
where $\complementary{y}$ is the complementary vector of $y$.

A probability distribution $\D$ on $\{0, 1\}^n$ is \emph{symmetric} if it has homogeneous symmetry
and it has complementary symmetry. 
\end{definition}

Fix $n$ and $\eps$, and let $X^+_\eps = \{x \in \{0, 1\}^n | w_1(x) \geq (\frac{1}{2}+\eps )n \}$
and $X^-_\eps = \{x \in \{0, 1\}^n | w_1(x) \leq (\frac{1}{2}-\eps )n \}$.
For $\calD$ a probability distribution on $\{0, 1\}^n$ for which
the events $x \in X^+_\eps$ and $x \in X^-_\eps$ have nonzero probability (where $x$
is the string of $n$ bits sampled by the distribution), 
denote by $\calD^+$ the probability distribution conditioned on $x \in X^+_\eps$,
and by $\calD^-$ the probability distribution conditioned on $x \in X^-_\eps$.

\todowrite{define w1, etc in introduction}

\begin{lemma}
\label{lem:symproperties}
Let $\calD$ be a distribution on $\{0, 1\}^n$ which has homogeneous symmetry.

\begin{enumerate}
    \item The probability $F_{\calD}(y)$ only depends on the number of zeros and ones
    of $y$: $F_{\calD}(y) = F_{\D}(0^{w_0(y)}1^{w_1(y)})$. We will denote by $f_\D(a, b)$ the value of $F_{\D}(0^a 1^b)$.
    \item If the events $x \in X^+_\eps$ and $x \in X^-_\eps$ have nonzero probability
    (i.e. $\D^+$ and $\D^-$ are well-defined),
    \todopolish{Find a way to phrase this without the awkward variable $x$?}
    the distributions $\D^+$ and $\D^-$ have homogeneous symmetry. 
    \item If $\D^+$ and $\D^-$ are well-defined and $a \leq b$, then $f_{\D^+}(b, a) \leq f_{\D^+}(a, b)$.
    \item If $\D$ has complementary symmetry and $\D^+$ and $\D^-$ are well-defined, then  
    $f_{\D^+}(a, b) = f_{\D^-}(b, a)$ for all
    $a, b \geq 0$.
\end{enumerate}
\end{lemma}
\begin{proof}
   Let us prove the first item. For $|y| = 1$ it is clear that
   the value of $F_{\D}(y)$ only depends the number of zeros and ones of $y$, since
   $y = 0$ or $y = 1$.  Now we want to prove that for every $y$ with $|y| \geq 2$,
   $F_\D(y) = F_\D(0^{w_0(y)}1^{w_1(y)})$. To prove that, it is enough to prove that we
   can perform ``swaps'' for any substring $10$ in $y$, that is, that if $y = y_0 \concat 10 \concat y_1$,
   then $F_\D(y_0 \concat 10 \concat y_1) = F_\D(y_0 \concat 01 \concat y_1)$. By a sequence of such swaps, we can always reach the 
   string $0^{w_0(y)}1^{w_1(y)}$. This is proved as follows:  
   \begin{align*}
   F_\D(y_0 \concat 10 \concat y_1) & = \Prob{x[S] = y_0 \concat 10 \concat y_1} \\
                                    & = \Prob{x[S] = y_0 \concat 10 \concat y_1} 
                                    + \Prob{x[S] = y_0 \concat 00 \concat y_1}
                                    - \Prob{x[S] = y_0 \concat 00 \concat y_1} \\
                                    & = \Prob{x[S \setminus \{i\}] = y_0 \concat 0 \concat y_1}
                                    - \Prob{x[S] = y_0 \concat 00 \concat y_1} \\
                                    & = \Prob{x[S \setminus \{j\}] = y_0 \concat 0 \concat y_1}
                                    - \Prob{x[S] = y_0 \concat 00 \concat y_1} \\
                                    & = \Prob{x[S] = y_0 \concat 01 \concat y_1} 
                                    + \Prob{x[S] = y_0 \concat 00 \concat y_1}
                                    - \Prob{x[S] = y_0 \concat 00 \concat y_1} \\
                                    & = F_\D(y_0 \concat 01 \concat y_1),
   \end{align*}
   where $i$ and $j$ are the indices of $S$ corresponding to the $10$ subsequence. 

Second item:
\begin{align*}
    \Probsub{x \samplefrom \D^+}{x[S] = y} & = \Probsub{x \samplefrom \D}{x[S] = y | x \in X^+_\eps} \\
        & = \sum_{\substack{z\in X^+_\eps \\ z[S] = y}} \Probsub{x \samplefrom \D}{x = z} \\
        & = \sum_{\substack{z\in X^+_\eps \\ z[S] = y}} F_{\D}(0^{w_0(z)}1^{w_1(z)})
        && (\text{first item on set } S = [n])\\
        & = \sum_{\substack{z\in X^+_\eps \\ z[S'] = y}} F_{\D}(0^{w_0(z)} 1^{w_1(z)}) \\
        & = \Probsub{x \samplefrom \D^+}{x[S'] = y}.
\end{align*}

Third item:
    \begin{align*}
        f_{\D^+}(b, a) & = \Probsub{x \samplefrom \D^+}{x[[a+b]] = 0^b1^a} \\
        & = \frac{1}{\binom{n}{a+b}\binom{a+b}{b}} \sum_{S \in \binom{[n]}{a+b}} 
        \sum_{\substack{y \in \{0, 1\}^{a+b}\\w_0(y)=b}} 
        \Probsub{x \samplefrom \D^+}{x[S] = y}
        && (\text{homog. sym.}) \\
        & = \frac{1}{\binom{n}{a+b}\binom{a+b}{b}} 
        \sum_{S \in \binom{[n]}{a+b}} 
        \sum_{\substack{y \in \{0, 1\}^{a+b}\\w_0(y)=b}} 
        \sum_{\substack{z\in X^+_\eps\\z[S]=y}} \Probsub{x \samplefrom \D^+}{x = z} \\
        & = \frac{1}{\binom{n}{a+b}\binom{a+b}{b}}
        \sum_{z \in X^+_\eps} \binom{w_0(z)}{b} \binom{w_1(z)}{a} \Probsub{x \samplefrom \D^+}{x = z}
        && (\text{swap sums}) \\
        & \leq \frac{1}{\binom{n}{a+b}\binom{a+b}{b}}
        \sum_{z \in X^+_\eps} \binom{w_0(z)}{a} \binom{w_1(z)}{b} \Probsub{x \samplefrom \D^+}{x = z}
        && (*) \\
        & = f_{\D^+}(a,b),
    \end{align*}
    where the inequality at $(*)$ uses that if $x \leq y$ and $a \leq b$, then 
    $\binom{x}{a} \binom{y}{b} \geq \binom{x}{b} \binom{y}{a}$.

Fourth item:
\begin{align*}
    f_{\D^+}(a, b) & = \Probsub{x \samplefrom \D}{x[[a+b]] = 0^a1^b | x \in X^+_\eps} \\
    & = \frac{1}{\Probsub{x \samplefrom \D}{x \in X^+_\eps}}
    \sum_{\substack{y \in X^+_\eps\\y[[a+b]] = 0^a1^b}} \Probsub{x \samplefrom \D}{x = y} \\
    & = \frac{1}{\Probsub{x \samplefrom \D}{x \in X^-_\eps}}
    \sum_{\substack{y \in X^-_\eps\\y[[a+b]] = 1^a0^b}} \Probsub{x \samplefrom \D}{x = y} 
    && (\text{comp. sym.})\\
    & = \Probsub{x \samplefrom \D}{x[[a+b]] = 1^a0^b | x \in X^-_\eps} = f_{\D^-}(b, a).
\end{align*}


\end{proof}



%\begin{lemma}
%\label{lem:comsymproperties}
%Let $\D$ be a symmetric distribution on $\{0, 1\}^n$ for which the distributions
%$\D^+$ and $\D^-$ are well-defined.
%Then:
%\begin{enumerate}
%\item

%\end{enumerate}
%\end{lemma}

%\begin{proof}
%\end{proof}


\begin{lemma}
\label{lem:symmetricbound}
Let $\calD$ be a symmetric distribution supported on $X^+_\eps \cup X^-_\eps$,
 and let $\A$ be a probabilistic query algorithm
that always does $t$ queries and solves $\MAJ$ with error probability $\delta$ 
on inputs sampled from $\calD$. 
Then:
$$
\sum_{i=0}^{\lceil t/2 \rceil-1} \binom{t}{i} f_{\D^+}(i, t-i) \leq \delta.
$$
\end{lemma}
\begin{proof}
Denote by $x$ the input, and let $Y = y_1 \ldots y_t$ denote the random variable corresponding
to the values of the $t$ sampled points. Note that $Y$ depends on $x$ and also on the randomness
over the execution of $Y_r$ be the random variable corresponding to the sampled points where
the randomness of $\A$ has been fixed to be $r$. $Y_r$ only depends on the choice of $x$.

Let $P^+ = \Prob{\A(x) = 0 | x \in X^+_\eps}$ the probability that the algorithm errs
conditioned on $x$ having a majority of ones, and similarly let 
$P^- = \Prob{\A(x) = 1 | x \in X^-_\eps}$ be the probability that the algorithm errs conditioned
on $x$ having a majority of zeros.

We have:

$$
P^+ = \sum_{y\in \{0, 1\}^t} \Prob{Y = y | x \in X^+_\eps} \Prob{\A(x) = 0 | Y = y, x \in X^+_\eps}.
$$

Now note the following:

\begin{itemize}
    \item Once $y$ is fixed, the result of $\A$ does not depend on $x$, but only on $y$ and $r$.
    Hence $\Probsub{x,r}{A(x) = 0 | Y = y, x \in X^+_\eps} = \Probsub{r}{\A(x) = 0 | Y = y}$.
    We denote this probability by $G_\A(y)$.
    \item Since $\D$ is symmetric, the event $Y = y$ does not depend on the
    randomness $r$ of the algorithm and in fact $\Prob{Y = y | x \in X^+_\eps} = F_{\D^+}(y)$.
    This is because, fixing some randomness $r$ in the algorithm, the event that the sequence
    of $t$ sample points is equal to $y$ is equivalent to the event that a subsequence of
    $x[S]$ of $x$ of size $t$ is equal to a particular permutation of $\sigma(y)$ of $y$. 
    By \cref{lem:symproperties} (1), this probability does not depend on $S$ or $\sigma$.
\end{itemize}

Thus, 

$$
P^+ = \sum_{y \in \{0, 1\}^t} F_{\D^+}(y) \cdot G_\A(y).
$$

Similarly:

$$
P^- = \sum_{y \in \{0, 1\}^t} F_{\D^-}(y) \cdot (1-G_\A(y)).
$$

Adding them and applying \cref{lem:symproperties}, we get the desired result:
\begin{align*}
\delta & \geq \frac{1}{2}\left(P^+ + P^-\right) \\
        & = \frac{1}{2}\sum_{y \in \{0, 1\}^t} F_{\D^+}(y) \cdot G_\A(y) + F_{\D^-}(y) \cdot (1-G_\A(y)) \\ 
        & \geq \frac{1}{2}\sum_{y \in \{0, 1\}^t} \min \{F_{\D^+}(y), F_{\D^-}(y)\} \\
        & = \frac{1}{2}\sum_{i=0}^{t} \binom{t}{i} \min \{f_{\D^+}(i, t-i), f_{\D^+}(t-i, i)\} 
        && (\text{\cref{lem:symproperties} (4)})\\
        & \geq \sum_{i=0}^{\lceil t/2 \rceil-1} \binom{t}{i} f_{\D^+}(i, t-i).
        && (\text{\cref{lem:symproperties} (3)})
\end{align*}

\end{proof}

\begin{theorem}
\label{thm:querylb}
\todo{conditions on eps}
Let $\A$ be a query algorithm that solves the $\MAJ$ problem on inputs uniformly drawn from 
$S_n = \{x \in \{0, 1\}^n | w_1(x) \in \{\lfloor(1/2-\eps)n\rfloor, \lceil(1/2+\eps)n\rceil\}\}$ 
with failure probability $\delta(n)$ using always at most $t(n)$ queries, with $t(n) \leq \frac{1}{4}\sqrt{n}$. 
There exists an absolute constant $C$ so that: 
$$
t(n) \geq \frac{1}{4\eps^2} \log \left(\frac{C}{\delta}\right)
$$

for all sufficiently large $n$.
\end{theorem}

\begin{proof}

First, we can assume that the algorithm always samples exactly $t(n)$ distinct points. Otherwise,
we can create an algorithm $A'$ which samples additional points before answerting
until exactly $t(n)$ points have been sampled.

For each $n$, the uniform distribution $U_{S_n}$ on $S_n$ is a symmetric distribution. 
Therefore, by \cref{lem:symmetricbound}, we have:

$$
\sum_{i=0}^{\lceil t/2 \rceil-1} \binom{t}{i} f_{U^+_{S_n}}(i, t-i) \leq \delta(n).
$$

Let $k = \lceil(1/2+\eps)n\rceil$. We can compute $f_{U^+_{S_n}}$:


\begin{equation}
\label{eq:fcalc}
f_{U^+_{S_n}}(w, t-w) = \frac{\binom{n-t}{k-w}}{\binom{n}{k}} = \frac{(n-t)!k!(n-k)!}{n!(k-w)!(((n-k)-(t-w))!}
    = \frac{(k)_w}{(n)_w} \frac{(n-k)_{t-w}}{(n-w)_{t-w}}
\end{equation}

Where $(n)_h = \frac{n!}{h!}$ is the falling factorial. We use the following elementary inequalities 
for the falling factorial (which follow from the inequality $1+x \leq e^x$):

$$
n^h e^{-\frac{h^2}{2(n-h)}} \leq (n)_h \leq n^h e^{-\frac{h(h-1)}{2n}}
$$

So we have:

$$
\frac{(k)_w}{(n)_w} \geq \frac{k^we^{-\frac{w^2}{2(n-w)}}}{n^we^{-\frac{w(w-1)}{2(n-w)}}} = \left(\frac{k}{n}\right)^w e^{-w\left(\frac{w}{2(k-w)}-\frac{w-1}{2n}\right)}
$$

Recall that $w \leq t \leq \frac{1}{4}\sqrt{n}$, and that $k > \frac{1}{2}n$. Using this, we can see that for sufficiently large $n$, we have that $e^{-w\left(\frac{w}{2(k-w)}-\frac{w-1}{2n}\right)} \geq 1/\sqrt{2}$, so:

$$
\frac{(k)_w}{(n)_w} \geq \frac{1}{\sqrt{2}}  \left(\frac{k}{n}\right)^w
$$

Similarly, for sufficiently large $n$ we have:

$$
\frac{(n-k)_{t-w}}{(n-w)_{t-w}} \geq \frac{1}{\sqrt{2}} \left(\frac{n-k}{n-w} \right)^{t-w} > \frac{1}{\sqrt{2}} \left(\frac{n-k}{n} \right)^{t-w}
$$

Substituting in \eqref{eq:fcalc}, we have:

$$
f_{U^+_{S_n}}(w, t-w)  \geq \frac{1}{2} \left(\frac{k}{n}\right)^w \left(\frac{n-k}{n} \right)^{t-w}
$$
    
So that:

\begin{align*}
\delta & \geq \sum_{i=0}^{\lceil t/2 \rceil-1} \binom{t}{i} f_{U^+_{S_n}}(i, t-i) \\
       & \geq \sum_{i=0}^{\lceil t/2 \rceil-1} \frac{1}{2} \binom{t}{i} \left(\frac{k}{n}\right)^i \left(\frac{n-k}{n} \right)^{t-i} \\
       & = \frac{1}{2} \Probsub{X \sim \text{Bin}(t, k/n)}{X < \left\lceil \frac{t}{2} \right\rceil}
\end{align*}

By standard bounds on the tail of the binomial distribution \cite{Feller43},
if $n$ is large enough and $\eps$ is small enough there exists a constant $C$ so that 
$\Probsub{X \sim \text{Bin}(t, k/n)}{X < \left\lceil \frac{t}{2} \right\rceil} \geq C e^{-4\eps^2t}$.
Rearranging, we get our desired result. 

\end{proof}

\begin{corollary}
    \label{cor:querylbgen}
Let $\eps(n)$ be a function satisfying $\eps(n) = \omega(n^{-1/4})$ and $\eps(n) = o(1)$. Then, all query
algorithms $\A$ solving $\MAJ_{n,\eps}$ with success probability $2/3$ for all inputs do at least $\Omega(1/\eps(n)^2)$
queries in the worst case. 
\end{corollary}
\begin{proof}
Consider the query algorithm $\A^*$ consisting in repeating algorithm $\A$  multiple times and 
taking the majority answer, where the number of repetitions is a constant such
that the failure probability of $\A^*$ is strictly less than 
the constant $C$ from \cref{thm:querylb}. 
Algorithm $\A^*$ in particular has failure probability $\delta < C$ on inputs 
uniformly drawn from $S_n$, so we can apply \cref{thm:querylb} to show that
$\A^*$ does $\Omega(1/\eps^2)$ queries in the worst case. Note that we use $\eps(n) = \omega(n^{-1/4})$
for this.
Since $\A^*$ repeats $\A$ a constant number of times, $\A$ does $\Omega(1/\eps^2)$ queries in the worst case. 
\end{proof}

\subsection{Randomized Constructive Separation}

\cref{thm:querylb} shows that uniformly sampling from $S_n$ results in inputs for which 
any algorithm with high success probability requires $\Omega(1/\eps^2)$ queries. 
However, in order to have a $\BPP$-constructive separation as in \cref{def:constructiveseparation},
we need to address some details regarding amplifying probabilities. 

\begin{itemize}
    \item Recall that we only assume that the algorithms have a probability gap of $1/3$. 
    \cref{thm:querylb} does not allow algorithms using $o(1/\eps^2)$ queries
    to have arbitrarily large probability gaps, but is perfectly consistent with an algorithm
    using $o(1/\eps^2)$ queries and giving the correct answer with 
    probability $2/3$. This is solved by refuting an amplified version of the algorithm as 
    in \cref{cor:querylbgen}.
    \item Recall that in \cref{def:refuter} we require randomized refuters to output
    counterexamples with probability at least $2/3$. \cref{thm:querylb} does not assure
    that a random sample from $S_n$ will be a counterexample with high probability: 
    Indeed, the algorithm that always answers $1$ will be correct with probability $1/2$ 
    on a uniformly random input from $S_n$. Therefore, our refuter needs to amplify this probability.
\end{itemize}

\begin{theorem}
\label{thm:qarandomizedcs}
Let $\eps(n)$ be a function satisfying $\eps(n) = \omega(n^{-1/4})$ and $\eps(n) = o(1)$. 
There is a $\BPP$-constructive separation of $\MAJ_{n, \eps}$ from randomized query algorithms
using $o(\eps^{-2})$ queries and running in $\poly(n)$ time. 
\end{theorem}
\begin{proof}
    Fix two arbitrary constants $\delta_0, \delta_1$ with $0 < 4 \cdot \delta_1 < \delta_0 < \min \{C, 1/3\}$, 
    for the constant $C$ of \cref{thm:querylb}.
    Let $\A$ be the algorithm being refuted, let $\determine{\A}$ be the determination
    of $\A$, and denote by $L(x)$ be the answer of $\MAJ_{n, \eps}$ on input $x$. 
    Consider the query algorithm $\A^*$ consisting in repeating algorithm $\A$  multiple times and 
    taking the majority answer, where the number of repetitions is a constant such
    that for all $x$ $\Probsub{r}{\A^*(x) \neq \determine{A}(x)} < \delta_1$, where the 
    probability is over the randomness $r$ in the execution of $\A^*$. 
    By \cref{thm:querylb},
    for $n$ large enough we have that $\Probsub{x \samplefrom U_{S_n}, r}{\A^*(x) \neq L(x)} \geq \delta_0$.

    Then:
    \begin{align*}
    \delta_0  \le & \,  \Prob{\A^*(x) \neq L(x)} \\
          = & \,  \Prob {\A^*(x) \neq L(x) | \determine{\A}(x) = L(x)} \Prob {\determine{\A}(x) = L(x)} \\
         &  + \Prob {\A^*(x) \neq L(x) | \determine{\A}(x) \neq L(x)} \Prob {\determine{\A}(x) \neq L(x)} \\
         < & \, \delta_1 + \Prob {\determine{\A}(x) \neq L(x)}.
    \end{align*}

    Therefore, $\Prob {\determine{\A}(x) \neq L(x)} > (\delta_0 - \delta_1) > 3 \delta_1$. Our refuter works
    as follows: it repeatedly samples $x \samplefrom U_{S_n}$ and simulates $\A^*$ on input $x$ until it finds an
    $x$ such that the output of $\A^*$ on $x$ is different from $L(x)$, and outputs that $x$. 
    
    The probability that our refuter outputs a correct counterexample is bounded below by the probability 
    that the event $\determine{\A}(x) \neq L(x)$, which occurs with probability greater than $3 \delta_1$,
    happens before the event $\A^*(x) \neq \determine{\A}(x)$, which occurs with probability at most $\delta_1$. 
    This probability is at least 
    $\frac{3\delta_1 (1-\delta_1)}{3\delta_1 + \delta_1 - (3\delta_1) \cdot \delta_1} 
    = \frac{3 (1-\delta_1)}{3(1-\delta_1)+ 1} > \frac{2}{3}$. \todo{verify this calculation}

        
\end{proof}



\section{Sufficiently Constructive Separations Imply Lower Bounds}

Recall that in \cite{ConstructiveSeparations}, the following theorem is proved: 

\thmcsrefuterqa*

From the proof we can get a stronger statement, with several improvements that are 
relevant for studying the best possible refuters in this setting:

\begin{itemize}
    \item The refuters can be quasipolynomial-size circuits (that is, $\qAC^0$ and 
    $\qNC^1$ rather than $\AC^0$ and $\NC^1$).
    \item The refuters can be quasipolynomial-length list-refuters. 
    \item It is only necessary to refute algorithms making $O(1/\eps^{1+\delta})$ queries,
    for some $\delta > 0$.
    \item It is only necessary to refute algorithms which are uniform distributions of
    $k$-juntas (not general randomized query algorithms).
\end{itemize}

\todo{Review this after definitions of refuters}

\begin{theorem}
    Let $\eps$ be a function of $n$ satisfying $\eps \leq 1/(\log n)^{\omega(1)}$, and $1/\eps$ is a positive integer
    computable in $\poly(1/\eps)$ time given $n$ in binary. Let $\delta > 0$, and let $t = 1/\eps^{1+\delta}$.
	\begin{itemize}
		\item If for every uniform distribution of $t$-juntas running in $\poly(1/\eps)$ time there exist
        a polylogtime-uniform-$\qAC^0$ quasipolynomial-length list-refuter $R$ of $\MAJ_{n, \eps}$ against 
        $\A$, then $\NP \neq \PTIME$. 
		\item If for every uniform distribution of $t$-juntas running in $\poly(1/\eps)$ time there exist
        a polylogtime-uniform-$\qNC^1$-uniform quasipolynomial-length list-refuter $R$ of $\MAJ_{n, \eps}$ against 
        $\A$, then $\PSPACE \neq \PTIME$. 
	\end{itemize}
\end{theorem}

\begin{proof}
    We will prove only the first item. The proof of the second item is analogous using the 
    corresponding second item from \cref{lem:acbinarytounary}.
    Assuming $\PTIME = \NP$, we will construct an algorithm $\A$ running in $\poly(1/\eps)$ time
    which samples $t$ points uniformly at random and solves the $\MAJ_{n, \eps}$ on the
    inputs generated by any polylogtime-uniform-$\qAC^0$ list-refuter.

    At the beginning, $\A$ computes $\eps$ in $\poly(1/\eps)$ time. 
    From an argument similar to \cref{lem:acbinarytounary}, the output
    $R(1^n)$ has circuit complexity $\polylog(n)$ (the function $f_R(m, i)$ now has 
    inputs of size $\polylog(n)$, but otherwise the proof of \cref{lem:acbinarytounary}
    is not modified).

    Therefore, there is a circuit of size $(c \log n)^c$ for some constant $c$ which computes
    the input given the index $i$ in binary. Since $\eps \leq 1/(\log n)^{\omega(1)}$,
    $(c \log n)^c \leq 1/\eps^{\delta/2}$ for sufficiently large $n$. 
    The number of circuits of size at most $1/\eps^{\delta/2}$ is $2^{O(\eps^{-\delta/2} \log \eps^{-1})}$.
    Therefore, this circuit can be PAC-learned with error $\eps/2$ and failure probability 
    $p = 1/6$ using $O(\eps^{-1} \cdot (\eps^{-\delta/2} \log \eps^{-1} + \log p^{-1})) \leq O(\eps^{-(1+\delta)})$
    random samples \cite[Theorem 2.5]{Mohri18}. Given that $\PTIME = \NP$, the learning of the circuit 
    $C$ can be done in $\poly(t, \log n) = \poly(1/\eps)$ time. Let $D$ be the resulting circuit.

    We decide $\MAJ_{n, \eps/2}$ on the output of $D$ by sampling $\Theta(1/\eps^2)$ uniformly random
    inputs from $D$, so that the probability of success is at least $5/6$, and return the result 
    as our answer. This takes $\poly(1/\eps)$ time. 
    Since the circuit $D$ only has error $\eps/2$ 
    compared with the original input string, the answer of $\MAJ_{n, \eps}$ on the original input
    string is the same as the answer of $\MAJ_{n, \eps/2}$ on the output of $D$.
    
    In total, our algorithm has success probability $2/3$, time complexity $\poly(1/\eps)$, and 
    sample complexity $O(1/\eps^{1+\delta})$. 
\end{proof}

\section{Constructive Separations Against Weaker Query Algorithms}
\label{sec:refuteragainstweakerqa}

In \cref{thm:csrefuterqa}, there are two important quantitative assumptions
that must hold in order to obtain a lower bound:

\begin{enumerate}
    \item We must have a refuter against all $t$-juntas with $t = O(1 / \eps^{1+\delta})$ for
    some $\delta > 0$.
    \item The function $\eps(n)$ must go to zero sufficiently fast: $\eps(n) = 1/(\log n)^{\omega(1)}$.
\end{enumerate}

We can wonder whether those conditions are \emph{tight}, in the sense that if we relax them slightly 
then we can have a constructive separation. We will show that the answer is positive for both of
those conditions. 

\subsection{Refuters for $t \leq C/\eps$}

In \cref{thm:csrefuterqa}, we need a refuter against $t$-juntas for $t = \Omega(1/\eps^{1+\delta})$ for any $\delta > 0$ in order 
to obtain a breakthrough. This condition is tight in the sense that we can easily construct list-refuters
for any query algorithms making less than
$C/\eps$ queries (for some $C$ depending on the probability gap of the algorithms).

\begin{theorem}
    \label{thm:refuterlessqueries}
Let $\eps(n)$ be a function so that $\eps(n) = o(1)$, $\eps(n) > 1/n$ and
 the integer $1/\eps$ is computable in polynomial time in $n$. 
There exists a polynomial-time 
algorithm $R$ that, on input $1^n$,
outputs a list of $O(1/\eps)$ strings of length $n$, 
so that every query algorithm using $\frac{1}{16\eps}$ queries
fails on one of the strings in the list with probability at least $1/3$.
\end{theorem}
\begin{proof}
The algorithm $R$ outputs $1 + \left\lfloor \frac{n}{4 \lceil \eps n \rceil}\right\rfloor$ strings.

The first string consist of $\lfloor (1/2 - \eps)n \rfloor$ ones, followed by zeros. 
The $i$-th next string ($1 \leq i \leq \left\lfloor\frac{n}{4 \lceil \eps n \rceil}\right\rfloor$)
consists of $\lfloor (1/2-\eps)n \rfloor$ ones followed by zeros in all positions except for 
the block from position
$\lfloor (1/2)n \rfloor +  (i-1) \cdot 2 \lceil \eps n \rceil$ to 
$\lfloor (1/2)n \rfloor +  i \cdot 2 \lceil \eps n \rceil - 1$.
That is, each string has a block of $2\lceil \eps n \rceil$ consecutive ones on its right half,
disjoint with the blocks of all other strings. 

Let $\A$ be any query algorithm as in the statement, and consider its execution on the first string.
With any fixed randomness $r$ for which $\A$ answers correctly on that string, 
it will answer incorrectly on at least half of the remaining 
$\left\lfloor \frac{n}{4 \lceil \eps n \rceil}\right\rfloor < \frac{1}{8\eps}$ strings,
since $\A$ can hit at most $\frac{1}{16\eps}$ distinct blocks with its queries.
Therefore, if $\A$ answers correctly on the first string with probability at least $2/3$, 
it will answer incorrectly with probability at least $1/3$ on at least one of the other strings.  

\end{proof}

Note that this is an ``explicit obstructions'' result: the list refuter does not depend on the algorithm being refuted. 
Note also that the refuter works for any query algorithm, not just juntas, and also
the refuter is quite simple and therefore can be implemented in restricted circuit models if
they allow computation of $1/\eps$ and simple arithmetic operations with it. 

\subsection{Derandomization of Query Algorithms}

One possible approach to obtaining deterministic constructive
separations is to try to derandomize 
the constructive separation of \cref{thm:qarandomizedcs}.
Pseudorandom generators fooling query algorithms are well-studied
\cite{Hatami2023}, and they will allow us to obtain a quasipolynomial
list-refuter when $\eps = 1/(\log(n))^{O(1)}$.

\begin{definition}
A set $X_1, \ldots, X_n$ of random variables is $k$-wise independent if for any $I \subseteq [n]$
with $|I| = k$ and any values $\{x_i\}_{i\in I}$, we have that the events $\{X_i = x_i\}_{i \in I}$ are independent.

We say that a probability distribution $\D$ on $\{0, 1\}^n$ is $k$-wise independent if the $n$ random variables 
corresponding to each of the bits are $k$-wise independent. 

We say that a probability distribution $\D$ on $\{0, 1\}^n$ is $k$-wise independent probability $p$ Bernoulli
if it is $k$-wise independent and $\Prob{x_i = 1} = p$ for all $i =1 , \dots, n$. 
\end{definition}

It is clear that $k$-juntas can not distinguish between $k$-wise independent Bernoulli distributions 
and $n$ independent Bernoulli random variables with the same probability. Interestingly, 
these distributions also fool any query algorithm using $k$ queries. 

\todowrite{Definitions of query algorithms in introduction}

\begin{theorem}
Let $\A$ be a query algorithm using at most $k$ queries on $\{0, 1\}^n$, 
let $\D$ be the distribution of $n$ independent 
Bernoulli variables with probability $p$ on $\{0, 1\}^n$, and let
$\D'$ be a $k$-wise independent probability $p$ Bernoulli distribution on $\{0, 1\}^n$. Then:
$$
\Probsub{x \samplefrom \D}{\A(x) = 1} = \Probsub{x \samplefrom \D'}{\A(x) = 1}.
$$
\end{theorem}
\begin{proof}
Note that it is sufficient to prove the result assuming $\A$ is a (deterministic) depth-$k$ decision tree,
since if $\A$ is probabilistic, the result for each of the decision trees resulting from each fixed randomness
implies the desired result for the probabilistic algorithm.

Let $\mathcal{L}$ be the set of leaves of the decision tree, and for each $u \in \mathcal{L}$,
let $\A_u(x)$ be equal
to $1$ if $\A(x) = 1$ and $\A$ ends at leaf $u$ of the tree on input $x$, and $0$ otherwise.
Note that $\A_u(x)$ is a $k$-junta
and that $\A(x) = \sum_{u\in \mathcal{L}} \A_u(x)$. Therefore:
$$
\EVsub{x \samplefrom \D}{\A(x)} = \sum_{u \in \mathcal{L}} \EVsub{x \samplefrom \D}{\A_u(x)} = \sum_{u \in \mathcal{L}} \EVsub{x \samplefrom \D'}{\A_u(x)} = \EVsub{x \samplefrom \D'}{\A(x)},
$$
and we have the desired result since  $\Prob{\A(x) = 1} = \EV{\A(x)}$,
because $\A(x)$ takes values $0$ and $1$.
\end{proof}

Now, we show that there is an efficient pseudorandom generator which samples from a $k$-wise
independent distribution with seed length $O(k \log n)$.

\begin{theorem}
\label{thm:kwiseindepgen}
There is a polynomial-time algorithm $\A$ which, given as input integers $n$, $k$ with $n \geq k \geq 1$,
an integer $p$ with $0 \leq p \leq 2^{\lceil \log_2 n \rceil}$, and $s = k \lceil \log_2 n \rceil$
random bits, outputs $n$ bits with a $k$-wise independent probability $p/2^{\lceil \log_2 n \rceil}$ 
Bernoulli distribution.
\end{theorem}
\begin{proof}
Let $q = 2^{\lceil \log_2 n \rceil}$, and let $\F_q$ be the finite field with $q$ elements. 

Let $\mathcal{P}_{k-1}$ be the set of univariate polynomials of degree at most $k-1$ in $\F_q$,
and let $z_1, \ldots, z_n \in \F_q$ be distinct elements of the field. Define the function 
$G \colon \mathcal{P}_{k-1} \to \F^n_q$ by
$$
G(p) = (p(z_1), \ldots, p(z_n)).
$$
We claim that, when $p$ is sampled uniformly at random from $\mathcal{P}_{k-1}$, the distribution
of $G(p)$ is $k$-wise independent. This is because, for all $I \subset [n]$ with $|I| \leq k$ and 
all sequences of values $\{x_i\}_{i\in I}$ in $\F_q$, the number of $p \in \mathcal{P}_{k-1}$ so that 
$p(z_i) = x_i$ for all $i \in I$ is exactly $q^{k-|I|}$, since each polynomial $p \in \mathcal{P}_{k-1}$
is uniquely determined by its evaluation at $k$ distinct points and conversely each possible sequence 
of values at $k$ distinct points yields a unique polynomial. Thus:
$$
\Prob{\bigwedge_{i\in I} p(z_i) = x_i} = \frac{1}{q^{|I|}} = \prod_{i\in I} \Prob{p(z_i) = x_i},
$$
and therefore the $|I|$ events $p(z_i) = x_i$ are independent, as desired. 

Now we describe the generator $\A$. We interpret the seed as the encoding of $k$ elements of $\F_q$,
determining a polynomial $p \in \mathcal{P}_{k-1}$, and we set the output bit to be $1$ if 
$f(p(z_i)) \leq p$ and to be $0$ otherwise, where $f$ is some bijection from $\F_q$ to $\{1, \ldots, q\}$.
Therefore, each bit has probability $p/q$ of being equal to $1$, and the $k$-wise independence of the
distribution follows from the $k$-wise independence of $G(p)$. 
\end{proof}

And finally, we can use this pseudorandom generator to derandomize \cref{thm:qarandomizedcs}.
Note that we can not derandomize the part where we amplify the probability, since that would
require simulating the algorithm $\A$ being refuted, which requires additional randomness. 
We do not prove all of the details, since the argument is similar to that of \cref{thm:querylb}
but with a different probability distribution and with some additional complications. 

\begin{theorem}
    \label{thm:refutergreatereps}
Let $\eps = \eps(n) = 1/(\log n)^{O(1)}$ and let $\delta > 0$ be a sufficiently small constant.
\todopolish{Clarify value of delta like in previous results}
There exists a quasipolynomial-time quasipolynomial-list-refuter
$R$ for $\MAJ_{n,\eps}$ against all query algorithms using $t(n) = o(1/\eps^2)$ queries with probability gap bounded by $1-2\delta$.
\end{theorem}
\begin{proofsketch}
The refuter is as follows: 
On input $1^n$, set $k = 1/\eps^2$, $q = 2^{\lceil \log_2 n \rceil}$ and run the $k$-wise independent generator from \cref{thm:kwiseindepgen}
for all of the $2^{O(k \log n)} = 2^{\polylog(n)}$ seeds for each of 
$p = p_+ = \left\lceil q \cdot (1/2 + 2\eps) \right\rceil$ and
$p = p_- = \left\lfloor q \cdot (1/2 - 2\eps) \right\rfloor$. 
Output all the results which satisfy the at least $(1/2+\eps)n$ ones or at most $(1/2-\eps)n$ ones promise. 

Consider the uniform probability distribution on the outputs of the refuter $\D_1$. 
This distribution is statistically close to the uniform distribution $\D_2$ on all the outputs
of the generator, without filtering those that do not satisfy the promise, since there are 
very few that do not satisfy the promise 
(a fraction of around $\Probsub{X \sim \text{Bin}(n, p_+/q)}{X < (1/2+\eps)n}$).
In turn, distribution $\D_2$ is indistinguishable by algorithms making $t(n)$ queries
from the corresponding distribution $\D_3$ in which either all $n$ bits are sampled from
independent Bernoullis with probability $p_+/q$ or all $n$ bits are sampled from Bernoullis with
probability $p_-/q$, each of the two options with probability $1/2$. 
And finally, distribution $\D_3$ is statistically close to distribution $\D_4$, which is distribution 
$\D_3$ conditioned on the output satisfying the promise. We conclude that query algorithms using $t(n)$
queries can not distinguish between distributions $\D_1$ and $\D_4$ except with some small probability
(exponentially small in $n$, in fact). \todowrite{Verify this claim.}

Distribution $\D_4$ is a symmetric distribution on $X^+_\eps \cup X^-_\eps$, 
and with similar arguments as in \cref{thm:querylb} 
we can prove that query algorithms with error probability $2\delta$ with inputs 
over distribution $\D_4$ require $\Omega(1/\eps^2)$ queries. Therefore, algorithms
with probability gap bounded by $1-2\delta$ must fail at least on a fraction $\delta$ of
inputs from $\D_4$. Therefore, they must also fail at a constant fraction of inputs from $\D_1$
and in particular they will fail for at least one of the elements outputted by $R(1^n)$, for $n$ big enough.

\end{proofsketch}

\todowrite{Write conclusion relating it to observations of the previous section.}

\subsection{Limits of Explicit Obstructions}

One salient aspect of the list-refuters described 
in the two previous subsections is that they are explicit
obstructions: they do not depend on the algorithm being refuted. 
This is much more restrictive than the requirement to obtain
a breakthrough in \cref{thm:csrefuterqa}, in which the refuters are allowed to
depend on the algorithm being refuted. The following results displays
the limits of explicit obstructions for this problem:

\begin{theorem}
    \label{thm:explicitlimits}
Let $R$ be an algorithm that, on input $1^n$, outputs a list $L = L_n$ of strings of size $n$ in time $\poly(|L|)$.
There is a probabilistic query algorithm $\A$ running in time $\poly(n, |L|, 1/\eps)$ and making $O\left(\frac{\log (|L|) \cdot \log (\log (|L|))}{\eps}\right)$ queries
which correctly solves $\MAJ_{n, \eps}$ with probability at least $2/3$ on each of the outputs produced by $R$.
\end{theorem}
\begin{proof}

The algorithm $\A$ is described in \cref{alg:explicitlimits}. First, $\A$ computes $L$ and divides it into
$L^+$ and $L^-$, depending on whether the majority is $1$ or $0$. $L^+$ and $L^-$ will mantain at all times the elements
of the list that are consistent with the information obtained by the queries. If at any point
$L^+$ or $L^-$ becomes empty, the answer is determined and the algorithm can output it and stop. 

\begin{algorithm}
\caption{Algorithm for \cref{thm:explicitlimits}}\label{alg:explicitlimits}
\begin{algorithmic}
\input{algorithms/algorithmexplicitobs.tex}
\end{algorithmic}
\end{algorithm}
    

$\A$ computes $m^+$ and $m^-$, the bitwise majority of all the elements of $L^+$ and $L^-$, respectively.
That is, for each $i = 1, \ldots, n$, $m^+_i$ is equal to $1$ if there are more elements of $L^+$ with $i$-th bit
equal to $1$ than elements with $i$-th bit equal to $0$, and is equal to $0$ otherwise. If $m^+ \neq m^-$, $\A$
queries one of the bits where $m^+$ and $m^-$ differ, discards the elements of $L^+$ and $L^-$ inconsistent with 
this information and starts over again. Note that this will result in halving the size of $L^+$ or $L^-$. 

If $m^+ = m^-$, then $\A$ determines the sets of indices where both $L^+$ and $L^-$ have majority $0$ 
and where they both have majority $1$ and samples $\lceil (\log (6 \log |L|)) \cdot 1/\eps \rceil$ random points
from the biggest set of those two. If all the sampled points coincide with the majority value on that 
set, $\A$ outputs the answer corresponding to that value. Otherwise, $\A$ starts over again; note that in
this case, the sizes of both $L^+$ and $L^-$ are halved.  

We proceed to the analysis of the algorithm. We have to argue that it performs $O(\log |L| \log \log |L| / \eps)$ queries, and
that it answers correctly with probability at least $2/3$. The bound on the number of queries is clear: 
the algorithm performs at most $O(\log \log |L| /\eps)$ queries in each iteration of the main loop
and it does at most $2 \cdot \log |L|$ iterations, since in each iteration the size 
of $L^+$ or $L^-$ is halved and it exits the loop when one of them becomes empty.

Suppose that we have an $x$ with $\geq (1/2 + \eps)$ ones, and we will bound the probability that $\A$ outputs $0$ (the
case where $x$ has majority $0$ is symmetric).
It is clear that if $\A$ exits the loop because $L^-$ has become empty, then it always gives the correct answer.
Then, $\A$ can only give a wrong answer when it outputs $0$ after sampling $\log (6 \log |L|)/\eps$ points in $S_0$
and finding that they are all equal to $0$. 
Note that, if $|S_0| \geq 1/2$, then $x[S_0]$ must have
at least $\eps n$ bits equal to $1$, and therefore the probability of sampling a bit equal to $1$ is at least $\eps$.
Therefore, the probability that all sampled bits are equal to $0$ is at most 
$(1-\eps)^{\log (6 \log |L|)/\eps} \leq \frac{1}{6 \log |L|}$. 
Since the algorithm performs at most $2 \log |L|$ iterations,
by the union bound the probability that $\A$ outputs $0$
is at most $2 \log |L| \cdot \frac{1}{6 \log |L|} = \frac{1}{3}$, as desired. 



\end{proof}

%Note that if $\eps = 1/(\log n)^{\omega(1)}$ and $|L|$ is quasipolynomial, then $O(1/\eps + \log |L|) = O(1/\eps)$,
%so the explicit obstructions refuter of \cref{thm:refuterlessqueries} is essentially optimal in this setting,
%and for algorithms that make $\Omega(1/\eps)$ queries running in time $\poly(n, |L|, 1/\eps)$ there are no explicit obstructions. 

\cref{thm:explicitlimits} shows that any constructive separation for a class of query algorithms including the algorithm described in 
the statement must necessarily have refuters that depend on the algorithm being refuted. 
Note that if $\eps = 1/(\log n)^{\omega(1)}$ and $|L|$ is quasipolynomial, then $O((\log |L| \log \log |L|)/\eps) = o(1/\eps^{1+\delta})$ for all 
$\delta > 0$. 
This suggests that that refuters that would achieve the breakthroughs of \cref{thm:csrefuterqa},
if they exist, should depend on the algorithm being refuted.
However, there are some details that could potentially make this not be the case: recall that
in \cref{thm:csrefuterqa} we only require refuters against uniform distributions of $k$-juntas
running in time $\poly(1/\eps)$,
while in \cref{thm:explicitlimits} the algorithm is an adaptive
query algorithm running in time $\poly(n, |L|, 1/\eps)$. 

There seems to be some room for improvement in \cref{thm:explicitlimits}.
There is a $\log (1/\eps) \log \log (1/\eps)$ gap between the number of 
queries in the algorithms refuted by the explicit obstructions of \cref{thm:refuterlessqueries}
and the number of queries in the 
algorithm $\A$ of \cref{thm:explicitlimits}.
The repeated sampling of $O(\log \log |L|/\eps)$ points seems to be somewhat wasteful, and 
perhaps with a different approach we could be able to get an $\A$ which makes e.g. $O(1/\eps + \log |L|)$ queries, 
which would close this gap.
It also seems feasible to eliminate the adaptivity of the queries by allowing $m^+$ and $m^-$
to differ in a small number of bits.  

\begin{question}
What is the precise frontier of explicit obstructions for $\MAJ_{n, \eps}$? Can we improve \cref{thm:explicitlimits} or \cref{thm:refuterlessqueries}?
\end{question}