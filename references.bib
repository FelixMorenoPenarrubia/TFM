@article{ConstructiveSeparations,
   title={Constructive Separations and Their Consequences},
   volume={Volume 3},
   ISSN={2751-4838},
   url={http://dx.doi.org/10.46298/theoretics.24.3},
   DOI={10.46298/theoretics.24.3},
   journal={TheoretiCS},
   publisher={Centre pour la Communication Scientifique Directe (CCSD)},
   author={Chen, Lijie and Jin, Ce and Santhanam, Rahul and Williams, Ryan},
   year={2024},
   month=feb
  }

@inproceedings{Chen20,
author = {Chen, Lijie and Jin, Ce and Williams, R. Ryan},
title = {Sharp threshold results for computational complexity},
year = {2020},
isbn = {9781450369794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357713.3384283},
doi = {10.1145/3357713.3384283},
abstract = {We establish several “sharp threshold” results for computational complexity. For certain tasks, we can prove a resource lower bound of n  c  for c ≥ 1 (or obtain an efficient circuit-analysis algorithm for n  c  size), there is strong intuition that a similar result can be proved for larger functions of n, yet we can also prove that replacing “n  c ” with “n  c+ε” in our results, for any ε > 0, would imply a breakthrough n ω(1) lower bound. We first establish such a result for Hardness Magnification. We prove (among other results) that for some c, the Minimum Circuit Size Problem for (logn) c -size circuits on length-n truth tables (MCSP[(logn) c ]) does not have n 2−o(1)-size probabilistic formulas. We also prove that an n 2+ε lower bound for MCSP[(logn) c ] (for any ε > 0 and c ≥ 1) would imply major lower bound results, such as NP does not have n  k -size formulas for all k, and #SAT does not have log-depth circuits. Similar results hold for time-bounded Kolmogorov complexity. Note that cubic size lower bounds are known for probabilistic De Morgan formulas (for other functions). Next we show a sharp threshold for Quantified Derandomization (QD) of probabilistic formulas: (a) For all α, ε > 0, there is a deterministic polynomial-time algorithm that finds satisfying assignments to every probabilistic formula of n 2−2α−ε size with at most 2 n α  falsifying assignments. (b) If for some α, ε > 0, there is such an algorithm for probabilistic formulas of n 2−α+ε-size and 2 n α  unsatisfying assignments, then a full derandomization of  NC 1 follows: a deterministic poly-time algorithm additively approximating the acceptance probability of any polynomial-size formula. Consequently, NP does not have n  k -size formulas, for all k. Finally we show a sharp threshold result for Explicit Obstructions, inspired by Mulmuley’s notion of explicit obstructions from GCT. An explicit obstruction against  S(n)-size formulas is a poly-time algorithm A such that A(1 n ) outputs a list {(x  i ,f(x  i ))} i ∈ [poly(n)] ⊆ {0,1} n  \texttimes{} {0,1}, and every S(n)-size formula F is inconsistent with the (partially defined) function f. We prove that for all ε > 0, there is an explicit obstruction against n 2−ε-size formulas, and prove that there is an explicit obstruction against n 2+ε-size formulas for some ε > 0 if and only if there is an explicit obstruction against all polynomial-size formulas. This in turn is equivalent to the statement that E does not have 2 o(n)-size formulas, a breakthrough in circuit complexity.},
booktitle = {Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing},
pages = {1335–1348},
numpages = {14},
keywords = {quantified derandomization, hardness magnification, de morgan formulas},
location = {Chicago, IL, USA},
series = {STOC 2020}
}
@article{Canetti95,
title = {Lower bounds for sampling algorithms for estimating the average},
journal = {Information Processing Letters},
volume = {53},
number = {1},
pages = {17-25},
year = {1995},
issn = {0020-0190},
doi = {https://doi.org/10.1016/0020-0190(94)00171-T},
url = {https://www.sciencedirect.com/science/article/pii/002001909400171T},
author = {Ran Canetti and Guy Even and Oded Goldreich},
keywords = {Theory of computation, Sampling, Estimating, Randomness, Lower bounds},
abstract = {We show lower bounds on the number of sample points and on the number of coin tosses used by general sampling algorithms for estimating the average value of functions over a large domain. The bounds depend on the desired precision and on the error probability of the estimate. Our lower bounds match upper bounds established by known algorithms, up to a multiplicative constant. Furthermore, we give a non-constructive proof of existence of an algorithm that improves the known upper bounds by a constant factor.}
}

@article{Oliveira24,
  title = {Reverse Mathematics of Complexity Lower Bounds},
  journal = {Electronic Colloquium on Computational Complexity},
  year = {2024},
  author = {Lijie Chen and Jiatu Li and Igor C. Oliveira}
}

@INPROCEEDINGS{Atserias06,
  author={Atserias, A.},
  booktitle={21st Annual IEEE Conference on Computational Complexity (CCC'06)}, 
  title={Distinguishing SAT from polynomial-size circuits, through black-box queries}, 
  year={2006},
  volume={},
  number={},
  pages={8 pp.-95},
  keywords={Polynomials;Circuits;Cryptography;Algorithm design and analysis;Stress;Distributed computing;Computational modeling;Production;Computational complexity},
  doi={10.1109/CCC.2006.17}}

@inproceedings{Bogdanov10,
  author       = {Andrej Bogdanov and
                  Kunal Talwar and
                  Andrew Wan},
  editor       = {Andrew Chi{-}Chih Yao},
  title        = {Hard Instances for Satisfiability and Quasi-one-way Functions},
  booktitle    = {Innovations in Computer Science - {ICS} 2010, Tsinghua University,
                  Beijing, China, January 5-7, 2010. Proceedings},
  pages        = {290--300},
  publisher    = {Tsinghua University Press},
  year         = {2010},
  url          = {http://conference.iiis.tsinghua.edu.cn/ICS2010/content/papers/23.html},
  timestamp    = {Wed, 04 Sep 2019 15:35:45 +0200},
  biburl       = {https://dblp.org/rec/conf/innovations/BogdanovTW10.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{HopcroftUllman,
  author = {
    J. E. Hopcroft and J. D. Ullman
  },
  title = {
    Formal Languages and Their Relation to Automata
  },
  publisher = {
    Addison-Wesley
  },
  year = {
   1979
  }
}

@article{Tadaki10,
title = {Theory of one-tape linear-time Turing machines},
journal = {Theoretical Computer Science},
volume = {411},
number = {1},
pages = {22-43},
year = {2010},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2009.08.031},
url = {https://www.sciencedirect.com/science/article/pii/S0304397509006264},
author = {Kohtaro Tadaki and Tomoyuki Yamakami and Jack C.H. Lin},
keywords = {One-tape Turing machine, Crossing sequence, Finite state automaton, Regular language, One-way function, Low set, Advice, Many-one reducibility},
abstract = {A theory of one-tape two-way one-head off-line linear-time Turing machines is essentially different from its polynomial-time counterpart since these machines are closely related to finite state automata. This paper discusses structural-complexity issues of one-tape Turing machines of various types (deterministic, nondeterministic, reversible, alternating, probabilistic, counting, and quantum Turing machines) that halt in linear time, where the running time of a machine is defined as the length of any longest computation path. We explore structural properties of one-tape linear-time Turing machines and clarify how the machines’ resources affect their computational patterns and power.}
}

@article{Hartmanis68,
author = {Hartmanis, J.},
title = {Computational Complexity of One-Tape Turing Machine Computations},
year = {1968},
issue_date = {April 1968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/321450.321464},
doi = {10.1145/321450.321464},
abstract = {The quantitative aspects of one-tape Turing machine computations are considered. It is shown, for instance, that there exists a sharp time bound which must be reached for the recognition of nonregular sets of sequences. It is shown that the computation time can be used to characterize the complexity of recursive sets of sequences, and several results are obtained about this classification. These results are then applied to the recognition speed of context-free languages and it is shown, among other things, that it is recursively undecidable how much time is required to recognize a nonregular context-free language on a one-tape Turing machine. Several unsolved problems are discussed.},
journal = {J. ACM},
month = {apr},
pages = {325–339},
numpages = {15}
}

@InProceedings{Damgard90,
author="Damg{\aa}rd, Ivan Bjerre",
editor="Brassard, Gilles",
title="A Design Principle for Hash Functions",
booktitle="Advances in Cryptology --- CRYPTO' 89 Proceedings",
year="1990",
publisher="Springer New York",
address="New York, NY",
pages="416--427",
abstract="We show that if there exists a computationally collision free function f from m bits to t bits where m > t, then there exists a computationally collision free function h mapping messages of arbitrary polynomial lengths to t-bit strings.",
isbn="978-0-387-34805-6"
}

@InProceedings{Merkle90,
author="Merkle, Ralph C.",
editor="Brassard, Gilles",
title="One Way Hash Functions and DES",
booktitle="Advances in Cryptology --- CRYPTO' 89 Proceedings",
year="1990",
publisher="Springer New York",
address="New York, NY",
pages="428--446",
abstract="One way hash functions are a major tool in cryptography. DES is the best known and most widely used encryption function in the commercial world today. Generating a one-way hash function which is secure if DES is a ``good'' block cipher would therefore be useful. We show three such functions which are secure if DES is a good random block cipher.",
isbn="978-0-387-34805-6"
}

@article{Hennie65,
  author       = {F. C. Hennie},
  title        = {One-Tape, Off-Line Turing Machine Computations},
  journal      = {Inf. Control.},
  volume       = {8},
  number       = {6},
  pages        = {553--578},
  year         = {1965},
  url          = {https://doi.org/10.1016/S0019-9958(65)90399-2},
  doi          = {10.1016/S0019-9958(65)90399-2},
  timestamp    = {Fri, 12 Feb 2021 22:15:47 +0100},
  biburl       = {https://dblp.org/rec/journals/iandc/Hennie65.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{NW94,
title = {Hardness vs randomness},
journal = {Journal of Computer and System Sciences},
volume = {49},
number = {2},
pages = {149-167},
year = {1994},
issn = {0022-0000},
doi = {https://doi.org/10.1016/S0022-0000(05)80043-1},
url = {https://www.sciencedirect.com/science/article/pii/S0022000005800431},
author = {Noam Nisan and Avi Wigderson},
abstract = {We present a simple new construction of a pseudorandom bit generator. It stretches a short string of truly random bits into a long string that looks random to any algorithm from a complexity class C (e.g., P, NC, PSPACE, …) using an arbitrary function that is hard for C. This construction reveals an equivalence between the problem of proving lower bounds and the problem of generating good pseudorandom sequences. Our construction has many consequences. The most direct one is that efficient deterministic simulation of randomized algorithms is possible under much weaker assumptions than previously known. The efficiency of the simulations depends on the strength of the assumptions, and may achieve P = BPP. We believe that our results are very strong evidence that the gap between randomized and deterministic complexity is not large. Using the known lower bounds for constant depth circuits, our construction yields an unconditionally proven pseudorandom generator for constant depth circuits. As an application of this generator we characterize the power of NP with a random oracle.}
}

@inproceedings{IW97,
author = {Impagliazzo, Russell and Wigderson, Avi},
title = {P = BPP if E requires exponential circuits: derandomizing the XOR lemma},
year = {1997},
isbn = {0897918886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/258533.258590},
doi = {10.1145/258533.258590},
booktitle = {Proceedings of the Twenty-Ninth Annual ACM Symposium on Theory of Computing},
pages = {220–229},
numpages = {10},
location = {El Paso, Texas, USA},
series = {STOC '97}
}

@inproceedings{Impagliazzo94,
author = {Impagliazzo, Russell and Nisan, Noam and Wigderson, Avi},
title = {Pseudorandomness for network algorithms},
year = {1994},
isbn = {0897916638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/195058.195190},
doi = {10.1145/195058.195190},
booktitle = {Proceedings of the Twenty-Sixth Annual ACM Symposium on Theory of Computing},
pages = {356–364},
numpages = {9},
location = {Montreal, Quebec, Canada},
series = {STOC '94}
}

@article{Impagliazzo95,
  title={A personal view of average-case complexity},
  author={Russell Impagliazzo},
  journal={Proceedings of Structure in Complexity Theory. Tenth Annual IEEE Conference},
  year={1995},
  pages={134-147},
  url={https://api.semanticscholar.org/CorpusID:2154064}
}

@article{Andreev98,
author = {Andreev, Alexander E. and Clementi, Andrea E. F. and Rolim, Jos\'{e} D. P.},
title = {A new general derandomization method},
year = {1998},
issue_date = {Jan. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0004-5411},
url = {https://doi.org/10.1145/273865.273933},
doi = {10.1145/273865.273933},
abstract = {We show that quick hitting set generators can replace quick pseudorandom generators to derandomize any probabilistic two-sided error algorithms. Up to now quick hitting set generators have been known as the general and uniform derandomization method for probabilistic one-sided error algorithms, while quick pseudorandom generators as the generators as the general and uniform method to derandomize probabilistic two-sided error algorithms.Our method is based on a deterministic algorithm that, given a Boolean circuit C and given access to a hitting set generator, constructs a discrepancy set for C. The main novelty is that the discrepancy set depends on C, so the new derandomization method is not uniform (i.e., not oblivious).The algorithm works in time exponential in k(p(n)) where k(*) is the price of the hitting set generator and p(*) is a polynomial function in the size of C. We thus prove that if a logarithmic price quick hitting set generator exists then BPP = P.},
journal = {J. ACM},
month = {jan},
pages = {179–213},
numpages = {35},
keywords = {derandomization, Boolean circuits, BPP}
}

@article{Kannan82,
title = {Circuit-size lower bounds and non-reducibility to sparse sets},
journal = {Information and Control},
volume = {55},
number = {1},
pages = {40-56},
year = {1982},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(82)90382-5},
url = {https://www.sciencedirect.com/science/article/pii/S0019995882903825},
author = {R. Kannan},
abstract = {As remarked in Cook (“Towards a Complexity Theory of Synchronous Parallel Computation,≓ Univ. of Toronto, 1980), a nonlinear lower bound on the circuit-size of a language in P or even in NP is not known. The best known published lower bound seems to be due to Paul (“Proceedings, 7th ACM Symposium on Theory of Computing,≓ 1975). In this paper it is shown first that for each nonnegative integer k there is a language Lk in σ2 ⌢ π2 (of the Meyer and Stockmeyer (“Proceedings, 13th IEEE Symposium on Switching and Automata Theory,≓ 1972) hierarchy) which does not have O(nk)-size circuits. Using the same techniques, one is able to prove several similar results. For example, it is shown that for each nonnegative integer k, there is a language Lk in NP that does not have O(nk)-size uniform circuits. This follows as a corollary of a stronger result shown in the paper. This result like the others to follow is not provable by direct diagonalization. It thus points to the most interesting feature of the techniques used hereby using the polynomial-time hierarchy, they are able to prove results about NP that cannot seem to proved by direct diagonalization. Finally, it is noted that existence of “small circuits≓ is in suitable contexts equivalent to being reducible to sparse sets. Using this, one is able to prove, for example, that for any time-constructible superpolynomial function f (n), NTIME(f (n)) contains a language which is not many-to-one p-time reducible to any sparse set.}
}

@InProceedings{Dolev13,
author="Dolev, Shlomi
and Fandina, Nova
and Gutfreund, Dan",
editor="Spirakis, Paul G.
and Serna, Maria",
title="Succinct Permanent Is NEXP-Hard with Many Hard Instances",
booktitle="Algorithms and Complexity",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="183--196",
abstract="The main motivation of this work is to study the average case hardness of the problems which belong to high complexity classes. In more detail, we are interested in provable hard problems which have a big set of hard instances. Moreover, we consider efficient generators of these hard instances of the problems. Our investigation has possible applications in cryptography. As a first step, we consider computational problems from the NEXP class.",
isbn="978-3-642-38233-8"
}

@INPROCEEDINGS{Gutfreund05,
  author={Gutfreund, D. and Shaltiel, R. and Ta-Shma, A.},
  booktitle={20th Annual IEEE Conference on Computational Complexity (CCC'05)}, 
  title={If NP languages are hard on the worst-case then it is easy to find their hard instances}, 
  year={2005},
  volume={},
  number={},
  pages={243-257},
  keywords={Polynomials;Computer science;Cryptography;Computational complexity;Stress;Information technology;Scholarships;Time measurement;NP-complete problem},
  doi={10.1109/CCC.2005.11}}


@inproceedings{Hatami2023,
  title={Theory of unconditional pseudorandom generators},
  author={Hatami, Pooya and Hoza, William},
  booktitle={Electron. Colloquium Comput. Complex., TR23-019},
  year={2023}
}







@article{BakerGillSolovay75,
author = {Baker, Theodore and Gill, John and Solovay, Robert},
title = {Relativizations of the P = NP Question},
journal = {SIAM Journal on Computing},
volume = {4},
number = {4},
pages = {431-442},
year = {1975},
doi = {10.1137/0204037},

URL = { 
    
        https://doi.org/10.1137/0204037
    
    

},
    abstract = { We investigate relativized versions of the open question of whether every language accepted nondeterministically in polynomial time can be recognized deterministically in polynomial time. For any set X, let \$\mathcal{P}^X (\text{resp. }\mathcal{NP}^X )\$ be the class of languages accepted in polynomial time by deterministic (resp. nondeterministic) query machines with oracle X. We construct a recursive set A such that \$\mathcal{P}^A = \mathcal{NP}^A \$. On the other hand, we construct a recursive set B such that \$\mathcal{P}^B \ne \mathcal{NP}^B \$. Oracles X are constructed to realize all consistent set inclusion relations between the relativized classes \$\mathcal{P}^X \$, \$\mathcal{NP}^X \$, and co \$\mathcal{NP}^X \$, the family of complements of languages in \$\mathcal{NP}^X \$. Several related open problems are described. }
}

@article{AaronsonWigderson09,
author = {Aaronson, Scott and Wigderson, Avi},
title = {Algebrization: A New Barrier in Complexity Theory},
year = {2009},
issue_date = {February 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1942-3454},
url = {https://doi.org/10.1145/1490270.1490272},
doi = {10.1145/1490270.1490272},
abstract = {Any proof of P ≠ NP will have to overcome two barriers: relativization and natural proofs. Yet over the last decade, we have seen circuit lower bounds (e.g., that PP does not have linear-size circuits) that overcome both barriers simultaneously. So the question arises of whether there is a third barrier to progress on the central questions in complexity theory.In this article, we present such a barrier, which we call algebraic relativization or algebrization. The idea is that, when we relativize some complexity class inclusion, we should give the simulating machine access not only to an oracle A, but also to a low-degree extension of A over a finite field or ring.We systematically go through basic results and open problems in complexity theory to delineate the power of the new algebrization barrier. First, we show that all known nonrelativizing results based on arithmetization---both inclusions such as IP = PSPACE and MIP = NEXP, and separations such as MAEXP ⊄ P/poly---do indeed algebrize. Second, we show that almost all of the major open problems---including P versus NP, P versus RP, and NEXP versus P/poly---will require non-algebrizing techniques. In some cases, algebrization seems to explain exactly why progress stopped where it did: for example, why we have superlinear circuit lower bounds for PromiseMA but not for NP.Our second set of results follows from lower bounds in a new model of algebraic query complexity, which we introduce in this article and which is interesting in its own right. Some of our lower bounds use direct combinatorial and algebraic arguments, while others stem from a surprising connection between our model and communication complexity. Using this connection, we are also able to give an MA-protocol for the Inner Product function with O (√nlogn) communication (essentially matching a lower bound of Klauck), as well as a communication complexity conjecture whose truth would imply NL ≠ NP.},
journal = {ACM Trans. Comput. Theory},
month = {feb},
articleno = {2},
numpages = {54},
keywords = {query complexity, oracles, interactive proofs, communication complexity, arithmetization, Low-degree polynomials}
}

@article{RazborovRudich97,
title = {Natural Proofs},
journal = {Journal of Computer and System Sciences},
volume = {55},
number = {1},
pages = {24-35},
year = {1997},
issn = {0022-0000},
doi = {https://doi.org/10.1006/jcss.1997.1494},
url = {https://www.sciencedirect.com/science/article/pii/S002200009791494X},
author = {Alexander A Razborov and Steven Rudich},
abstract = {We introduce the notion ofnaturalproof. We argue that the known proofs of lower bounds on the complexity of explicit Boolean functions in nonmonotone models fall within our definition of natural. We show, based on a hardness assumption, that natural proofs can not prove superpolynomial lower bounds for general circuits. Without the hardness assumption, we are able to show that they can not prove exponential lower bounds (for general circuits) for the discrete logarithm problem. We show that the weaker class ofAC0-natural proofs which is sufficient to prove the parity lower bounds of Furst, Saxe, and Sipser, Yao, and Håstad is inherently incapable of proving the bounds of Razborov and Smolensky. We give some formal evidence that natural proofs are indeed natural by showing that every formal complexity measure, which can prove superpolynomial lower bounds for a single function, can do so for almost all functions, which is one of the two requirements of a natural proof in our sense.}
}

@article{Kabanets01,
title = {Easiness Assumptions and Hardness Tests: Trading Time for Zero Error},
journal = {Journal of Computer and System Sciences},
volume = {63},
number = {2},
pages = {236-252},
year = {2001},
issn = {0022-0000},
doi = {https://doi.org/10.1006/jcss.2001.1763},
url = {https://www.sciencedirect.com/science/article/pii/S0022000001917635},
author = {Valentine Kabanets},
abstract = {We propose a new approach toward derandomization in the uniform setting, where it is computationally hard to find possible mistakes in the simulation of a given probabilistic algorithm. The approach consists in combining both easiness and hardness complexity assumptions: if a derandomization method based on an easiness assumption fails, then we obtain a certain hardness test that can be used to remove error in BPP algorithms. As an application, we prove that every RP algorithm can be simulated by a zero-error probabilistic algorithm, running in expected subexponential time, that appears correct infinitely often (i.o.) to every efficient adversary. A similar result by Impagliazzo and Wigderson (FOCS'98) states that BPP allows deterministic subexponential-time simulations that appear correct with respect to any efficiently sampleable distribution i.o., under the assumption that EXP≠BPP; in contrast, our result does not rely on any unproven assumptions. As another application of our techniques, we get the following gap theorem for ZPP: either every RP algorithm can be simulated by a deterministic subexponential-time algorithm that appears correct i.o. to every efficient adversary or EXP=ZPP. In particular, this implies that if ZPP is somewhat easy, e.g., ZPP⊆DTIME(2nc) for some fixed constant c, then RP is subexponentially easy in the uniform setting described above.}
}

@article{Mulmuley10,
  title={Explicit Proofs and The Flip},
  author={Ketan Mulmuley},
  journal={ArXiv},
  year={2010},
  volume={abs/1009.0246},
  url={https://api.semanticscholar.org/CorpusID:18834159}
}

@article{Turing36,
  title={On computable numbers, with an application to the Entscheidungsproblem},
  author={Turing, Alan Mathison},
  journal={J. of Math},
  volume={58},
  number={345-363},
  pages={5},
  year={1936}
}

@article{Feller43,
  title={Generalization of a probability limit theorem of Cram{\'e}r},
  author={William Feller},
  journal={Transactions of the American Mathematical Society},
  year={1943},
  volume={54},
  pages={601-612},
  url={https://api.semanticscholar.org/CorpusID:121615635}
}

@book{Mohri18,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}

@book{AB09,
  title={Computational complexity: a modern approach},
  author={Arora, Sanjeev and Barak, Boaz},
  year={2009},
  publisher={Cambridge University Press}
}



  



  



