\todowritehp{Finish writing Chapter 3}

In this chapter, we study constructive separations in the query algorithm setting.
In particular, we study the problem $\MAJ$ of distinguishing determining whether
a binary string $x \in \{0, 1\}^n$ has at least $(\frac{1}{2}+\eps)n$ ones or
at most $(\frac{1}{2}-\eps)n$ ones, under the promise that it is one of the two
cases. 

We will prove the $\Omega(1/\eps^2)$ lower bound in the number of queries
for $\MAJ$, show a randomized constructive separation against algorithms
using fewer queries, and prove and discuss \cref{thm:csrefuterqa}, the result
from \cite{ConstructiveSeparations} showing that breakthrough lower
bounds follow from sufficiently constructive separations of $\MAJ$.  


\section{Lower Bounds Against Query Algorithms for $\MAJ$}

The obvious algorithm for $\MAJ$ consists in randomly sampling values and answering 
according to the majority of the values we have obtained. According to well-studied
bounds on the binomial distribution, $\Theta(1/\eps^2)$ samples
are necessary and sufficient to make the probability of error smaller than any 
fixed constant following this strategy.

In this section, we will prove the very intuitive result that this is essentially
the best that can be done, that is, that there is no query algorithm making $o(1/\eps^2)$
queries that gives the right answer with small probability of error. This was 
proved in \cite{Canetti95} in the more general context of \emph{samplers}: algorithms
that compute the average of a real-valued function $f \colon \{0, 1\}^n \to [0, 1]$
by sampling some of the inputs. 

The result that we will prove here is stronger than the one stated in \cite{Canetti95}:
we will prove not just that such query algorithms require $\Omega(1/\eps^2)$ in the 
worst case in order to have a high success probability \emph{for all} inputs; instead,
we will prove that $\Omega(1/\eps^2)$ queries are required for \emph{random} inputs,
where the probability of success is over the randomness of both the input and the algorithm.
This will allow us to obtain a constructive separation.
%However, the proof argument 
%we use is similar to the one used in \cite{Canetti95}.

\subsection{Proof of Lower Bounds for Random Inputs}

Fix $n$ and $\eps$, and let
$X^+_\eps = \{x \in \{0, 1\}^n | w_1(x) \geq (\frac{1}{2}+\eps )n \}$
and $X^-_\eps = \{x \in \{0, 1\}^n | w_1(x) \leq (\frac{1}{2}-\eps )n \}$.
\todowrite{define w1, etc in introduction}

For $\calD$ a probability distribution on $X^+_\eps \cup X^-_\eps$, denote
by $\calD^+$ the probability distribution conditioned on $x \in X^+_\eps$,
and by $\calD^-$ the probability distribution conditioned on $x \in X^-_\eps$.
We say that a probability distribution $\calD$ on $X^+_\eps \cup X^-_\eps$ is
symmetric if for every $y \in \{0, 1\}^*$ and every pair of subsets $S, S' \subset [n]$
with $|S| = |S'| = y$, 
$\Probsub{x \samplefrom \calD^+}{x[S] = y} = \Probsub{x \samplefrom \calD^+}{x[S'] = y}$ and
$\Probsub{x \samplefrom \calD^-}{x[S] = y} = \Probsub{x \samplefrom \calD^-}{x[S'] = y}$.
That is, the probability $\Probsub{x \samplefrom \calD^+}{x[S] = y}$ does not depend
on the set $S$: denote by $F_{\calD}^+(y)$ the probability
$\Probsub{x \samplefrom \D}{x[S] = y | x \in X^+_{\eps}}$ and by $F_{\calD}^-(y)$ the probability
$\Probsub{x \samplefrom \D}{x[S] = y | x \in X^-_{\eps}}$. 

\begin{lemma}
\label{lem:symmetricproperties}
Let $\calD$ be a symmetric distribution on $X^+_\eps \cup X^-_\eps$.

\begin{enumerate}
    \item The probabilities $F_{\calD}^+(y)$ and $F_{\calD}^-(y)$ only depend on the number of ones and zeros
    of $y$: $F_{\calD}^+(y) = f_{\D}^+(w_0(y), w_1(y)), F_{\D}^-(y) = f_{\D}^-(w_0(y), w_1(y))$.
    \item $f^+(a, b) = f^-(b, a)$ for all $a, b \geq 0$. 
    \item If $a \leq b$, then $f^-_\D(a, b) \leq f^+_\D(a, b)$.
\end{enumerate}
\end{lemma}
\begin{proof}
    TODO. \todowrite{prove symmetric distribution lemma}
\end{proof}

\begin{lemma}
\label{lem:symmetricbound}
Let $\calD$ be a symmetric distribution on $X^+_\eps \cup X^-_\eps$,
 and let $\A$ be a probabilistic query algorithm
that always does $t$ queries and solves $\MAJ$ with error probability $\delta$ 
on inputs sampled from $\calD$. 
Then:
$$
\sum_{i=0}^{\lceil t/2 \rceil-1} \binom{t}{i} f^+_{\D}(i, t-i) \leq \delta.
$$
\end{lemma}
\begin{proof}
Denote by $x$ the input, and let $Y = y_1 \ldots y_t$ denote the random variable corresponding
to the values of the $t$ sampled points. Note that $Y$ depends on $x$ and also on the randomness
over the execution of $Y_r$ be the random variable corresponding to the sampled points where
the randomness of $\A$ has been fixed to be $r$. $Y_r$ only depends on the choice of $x$.

Let $P^+ = \Prob{\A(x) = 0 | x \in X^+_\eps}$ the probability that the algorithm errs
conditioned on $x$ having a majority of ones, and similarly let 
$P^- = \Prob{\A(x) = 1 | x \in X^-_\eps}$ be the probability that the algorithm errs conditioned
on $x$ having a majority of zeros.

We have:

$$
P^+ = \sum_{y\in \{0, 1\}^t} \Prob{Y = y | x \in X^+_\eps} \Prob{\A(x) = 0 | Y = y, x \in X^+_\eps}.
$$

Now note the following:

\begin{itemize}
    \item Once $y$ is fixed, the result of $\A$ does not depend on $x$, but only on $y$ and $r$.
    Hence $\Probsub{x,r}{A(x) = 0 | Y = y, x \in X^+_\eps} = \Probsub{r}{\A(x) = 0 | Y = y}$.
    We denote this probability by $G_\A(y)$.
    \item Since $\D$ is symmetric, the event $Y = y$ does not depend on the
    randomness $r$ of the algorithm and in fact $\Prob{Y = y | x \in X^+_\eps} = F^+_{\D}(y)$.
    This is because, fixing some randomness $r$ in the algorithm, the event that the sequence
    of $t$ sample points is equal to $y$ is equivalent to the event that a subsequence of
    $x[S]$ of $x$ of size $t$ is equal to a particular permutation of $\sigma(y)$ of $y$. 
    By \cref{lem:symmetricproperties}, this probability does not depend on $S$ or $\sigma$.
\end{itemize}

Thus, 

$$
P^+ = \sum_{y \in \{0, 1\}^t} F^+_\D(y) \cdot G_\A(y).
$$

Similarly:

$$
P^- = \sum_{y \in \{0, 1\}^t} F^-_\D(y) \cdot (1-G_\A(y)).
$$

Adding them and applying \cref{lem:symmetricproperties}, we get the desired result:
\begin{align*}
\delta & \geq \frac{1}{2}\left(P^+ + P^-\right) \\
        & = \frac{1}{2}\sum_{y \in \{0, 1\}^t} F^+_\D(y) \cdot G_\A(y) + F^-_\D(y) \cdot (1-G_\A(y)) \\ 
        & \geq \frac{1}{2}\sum_{y \in \{0, 1\}^t} \min \{F^+_\D(y), F^-_\D(y)\} \\
        & = \frac{1}{2}\sum_{i=0}^{t} \binom{t}{i} \min \{f^+_\D(i, t-i), f^+_\D(t-i, i)\} \\
        & \geq \sum_{i=0}^{\lceil t/2 \rceil-1} \binom{t}{i} f^+_\D(i, t-i).
\end{align*}

\end{proof}

\begin{theorem}
\label{thm:querylb}
Let $\A$ be a query algorithm that solves the $\MAJ$ problem on inputs uniformly drawn from 
$S_n = \{x \in \{0, 1\}^n | w_1(x) \in \{\lfloor(1/2-\eps)n\rfloor, \lceil(1/2+\eps)n\rceil\}\}$ 
with failure probability $\delta(n)$ using always at most $t(n)$ queries, with $t(n) \leq \frac{1}{4}\sqrt{n}$. 
There exists an absolute constant $C$ so that: 
$$
t(n) \geq \frac{1}{4\eps^2} \log \left(\frac{C}{\delta}\right)
$$

for all sufficiently large $n$.
\end{theorem}

\begin{proof}

First, we can assume that the algorithm always samples exactly $t(n)$ distinct points. Otherwise,
we can create an algorithm $A'$ which samples additional points before answerting
until exactly $t(n)$ points have been sampled.

For each $n$, the uniform distribution $U_{S_n}$ on $S_n$ is a symmetric distribution. 
Therefore, by \cref{lem:symmetricbound}, we have:

$$
\sum_{i=0}^{\lceil t/2 \rceil-1} \binom{t}{i} f^+_{U_{S_n}}(i, t-i) \leq \delta(n).
$$

Let $k = \lceil(1/2+\eps)n\rceil$. We can compute $f^+_{U_{S_n}}$:


\begin{equation}
\label{eq:fcalc}
f^+_{U_{S_n}}(w, t-w) = \frac{\binom{n-t}{k-w}}{\binom{n}{k}} = \frac{(n-t)!k!(n-k)!}{n!(k-w)!(((n-k)-(t-w))!}
    = \frac{(k)_w}{(n)_w} \frac{(n-k)_{t-w}}{(n-w)_{t-w}}
\end{equation}

Where $(n)_h = \frac{n!}{h!}$ is the falling factorial. We use the following inequalities 
(TODO references \todonit{references for factorial inequalities})
    for the falling
factorial:

$$
n^h e^{-\frac{h^2}{2(n-h)}} \leq (n)_h \leq n^h e^{-\frac{h(h-1)}{2n}}
$$

So we have:

$$
\frac{(k)_w}{(n)_w} \geq \frac{k^we^{-\frac{w^2}{2(n-w)}}}{n^we^{-\frac{w(w-1)}{2(n-w)}}} = \left(\frac{k}{n}\right)^w e^{-w\left(\frac{w}{2(k-w)}-\frac{w-1}{2n}\right)}
$$

Recall that $w \leq t \leq \frac{1}{4}\sqrt{n}$, and that $k > \frac{1}{2}n$. Using this, we can see that for sufficiently large $n$, we have that $e^{-w\left(\frac{w}{2(k-w)}-\frac{w-1}{2n}\right)} \geq 1/\sqrt{2}$, so:

$$
\frac{(k)_w}{(n)_w} \geq \frac{1}{\sqrt{2}}  \left(\frac{k}{n}\right)^w
$$

Similarly, for sufficiently large $n$ we have:

$$
\frac{(n-k)_{t-w}}{(n-w)_{t-w}} \geq \frac{1}{\sqrt{2}} \left(\frac{n-k}{n-w} \right)^{t-w} > \frac{1}{\sqrt{2}} \left(\frac{n-k}{n} \right)^{t-w}
$$

Substituting in \eqref{eq:fcalc}, we have:

$$
f^+_{U_{S_n}}(w, t-w)  \geq \frac{1}{2} \left(\frac{k}{n}\right)^w \left(\frac{n-k}{n} \right)^{t-w}
$$
    
So that:

\begin{align*}
\delta & \geq \sum_{i=0}^{\lceil t/2 \rceil-1} \binom{t}{i} f^+_{U_{S_n}}(i, t-i) \\
       & \geq \sum_{i=0}^{\lceil t/2 \rceil-1} \frac{1}{2} \binom{t}{i} \left(\frac{k}{n}\right)^i \left(\frac{n-k}{n} \right)^{t-i} \\
       & = \frac{1}{2} \Probsub{X \sim \text{Bin}(t, k/n)}{X < \left\lceil \frac{t}{2} \right\rceil}
\end{align*}

By standard bounds on the tail of the binomial distribution,
(TODO reference and details \todonit{reference for tails of binomial distribution})
we have that there exists a constant $C$ so that 
$\Probsub{X \sim \text{Bin}(t, k/n)}{X < \left\lceil \frac{t}{2} \right\rceil} \geq C e^{-4\eps^2t}$.
Rearranging, we get our desired result. 

\end{proof}

\begin{corollary}
Let $\eps(n)$ be a function satisfying $\eps(n) = \omega(n^{-1/4})$ and $\eps(n) = o(1)$. Then, all query
algorithms $\A$ solving $\MAJ$ with bounded failure probability for all inputs do at least $\Omega(1/\eps(n)^2)$
queries. 
\end{corollary}
\begin{proof}
TODO \todowrite{prove corollary, and state it more precisely too}
\end{proof}

\subsection{Randomized Constructive Separation}

Explain how to make a randomized constructive separation from above (amplify error probability).

\begin{theorem}
\label{thm:qarandomizedcs}

\end{theorem}



\section{Sufficiently Constructive Separations Imply Lower Bounds}

State and prove possible strengthenings of the result of \cite{ConstructiveSeparations}. 

\thmcsrefuterqa*

\section{Constructive Separations Against Weaker Query Algorithms}

In \cref{thm:csrefuterqa}, there are two important quantitative assumptions
that must hold in order to obtain a lower bound:

\begin{enumerate}
    \item We must have a refuter against all $t$-juntas with $t = O(\log \eps^{-1} / \eps^{1+\delta})$ for
    some $\delta > 0$.
    \item The function $\eps(n)$ must go to zero sufficiently fast: $\eps(n) = 1/(\log n)^{\omega(1)}$.
\end{enumerate}

We can wonder whether those conditions are \emph{tight}, in the sense that if we relax them slightly 
then we can have a constructive separation. We will show that the answer is positive for both of
those conditions. 

\subsection{Refuters for $t \leq C/\eps$}

Explain that for query algorithms making $o(1/\eps)$ queries, explicit-obstructions refuters exist.

\subsection{Derandomization of Query Algorithms and Refuters for $\eps = 1/\polylog(n)$}

One possible approach to obtaining deterministic constructive
separations is to try to derandomize 
the constructive separation of \cref{thm:qarandomizedcs}.
Pseudorandom generators fooling query algorithms are well-studied
\cite{Hatami2023}, and they will allow us to obtain a quasipolynomial
list-refuter when $\eps = 1/(\log(n))^{O(1)}$.

\begin{definition}
A set $X_1, \ldots, X_n$ of random variables is $k$-wise independent if for any $I \subseteq [n]$
with $|I| = k$ and any values $\{x_i\}_{i\in I}$, we have that the events $\{X_i = x_i\}_{i \in I}$ are independent.

We say that a probability distribution $\D$ on $\{0, 1\}^n$ is $k$-wise independent if the $n$ random variables 
corresponding to each of the bits are $k$-wise independent. 

We say that a probability distribution $\D$ on $\{0, 1\}^n$ is $k$-wise independent probability $p$ Bernoulli
if it is $k$-wise independent and $\Prob{x_i = 1} = p$ for all $i =1 , \dots, n$. 
\end{definition}

\todopolish{Think of better/shorter terms to use to refer to k-wise independent prob p Bernoulli.}

It is clear that $k$-juntas can not distinguish between $k$-wise independent Bernoulli distributions 
and $n$ independent Bernoulli random variables with the same probability. Interestingly, 
these distributions also fool any query algorithm using $k$ queries. 

\todowrite{Definitions of query algorithms in introduction}

\begin{theorem}
Let $\A$ be a query algorithm using at most $k$ queries on $\{0, 1\}^n$, 
let $\D$ be the distribution of $n$ independent 
Bernoulli variables with probability $p$ on $\{0, 1\}^n$, and let
$\D'$ be a $k$-wise independent probability $p$ Bernoulli distribution on $\{0, 1\}^n$. Then:
$$
\Probsub{x \samplefrom \D}{\A(x) = 1} = \Probsub{x \samplefrom \D'}{\A(x) = 1}.
$$
\end{theorem}
\begin{proof}
Note that it is sufficient to prove the result assuming $\A$ is a (deterministic) depth-$k$ decision tree,
since if $\A$ is probabilistic, the result for each of the decision trees resulting from each fixed randomness
implies the desired result for the probabilistic algorithm.

Let $\mathcal{L}$ be the set of leaves of the decision tree, and for each $u \in \mathcal{L}$,
let $\A_u(x)$ be equal
to $1$ if $\A(x) = 1$ and $\A$ ends at leaf $u$ of the tree on input $x$, and $0$ otherwise.
Note that $\A_u(x)$ is a $k$-junta
and that $\A(x) = \sum_{u\in \mathcal{L}} \A_u(x)$. Therefore:
$$
\EVsub{x \samplefrom \D}{\A(x)} = \sum_{u \in \mathcal{L}} \EVsub{x \samplefrom \D}{\A_u(x)} = \sum_{u \in \mathcal{L}} \EVsub{x \samplefrom \D'}{\A_u(x)} = \EVsub{x \samplefrom \D'}{\A(x)},
$$
and we have the desired result since  $\Prob{\A(x) = 1} = \EV{\A(x)}$,
because $\A(x)$ takes values $0$ and $1$.
\end{proof}

Now, we show that there is an efficient pseudorandom generator which samples from a $k$-wise
independent distribution with seed length $O(k \log n)$.

\begin{theorem}
\label{thm:kwiseindepgen}
There is a polynomial-time algorithm $\A$ which, given as input integers $n$, $k$ with $n \geq k \geq 1$,
an integer $p$ with $0 \leq p \leq 2^{\lceil \log_2 n \rceil}$, and $s = k \lceil \log_2 n \rceil$
random bits, outputs $n$ bits with a $k$-wise independent probability $p/2^{\lceil \log_2 n \rceil}$ 
Bernoulli distribution.
\end{theorem}
\begin{proof}
Let $q = 2^{\lceil \log_2 n \rceil}$, and let $\F_q$ be the finite field with $q$ elements. 

Let $\mathcal{P}_{k-1}$ be the set of univariate polynomials of degree at most $k-1$ in $\F_q$,
and let $z_1, \ldots, z_n \in \F_q$ be distinct elements of the field. Define the function 
$G \colon \mathcal{P}_{k-1} \to \F^n_q$ by
$$
G(p) = (p(z_1), \ldots, p(z_n)).
$$
We claim that, when $p$ is sampled uniformly at random from $\mathcal{P}_{k-1}$, the distribution
of $G(p)$ is $k$-wise independent. This is because, for all $I \subset [n]$ with $|I| \leq k$ and 
all sequences of values $\{x_i\}_{i\in I}$ in $\F_q$, the number of $p \in \mathcal{P}_{k-1}$ so that 
$p(z_i) = x_i$ for all $i \in I$ is exactly $q^{k-|I|}$, since each polynomial $p \in \mathcal{P}_{k-1}$
is uniquely determined by its evaluation at $k$ distinct points and conversely each possible sequence 
of values at $k$ distinct points yields a unique polynomial. Thus:
$$
\Prob{\bigwedge_{i\in I} p(z_i) = x_i} = \frac{1}{q^{|I|}} = \prod_{i\in I} \Prob{p(z_i) = x_i},
$$
and therefore the $|I|$ events $p(z_i) = x_i$ are independent, as desired. 

Now we describe the generator $\A$. We interpret the seed as the encoding of $k$ elements of $\F_q$,
determining a polynomial $p \in \mathcal{P}_{k-1}$, and we set the output bit to be $1$ if 
$f(p(z_i)) \leq p$ and to be $0$ otherwise, where $f$ is some bijection from $\F_q$ to $\{1, \ldots, q\}$.
Therefore, each bit has probability $p/q$ of being equal to $1$, and the $k$-wise independence of the
distribution follows from the $k$-wise independence of $G(p)$. 
\end{proof}

And finally, we can use this pseudorandom generator to derandomize \cref{thm:qarandomizedcs}.
We do not prove all of the details, since the argument is similar to that of \cref{thm:querylb}
but with a different probability distribution and with some additional complications. 

\begin{theorem}
Let $\eps = \eps(n) = 1/(\log n)^{O(1)}$ and let $\delta > 0$ be a sufficiently small constant.
\todopolish{Clarify value of delta like in previous results}
There exists a quasipolynomial-time quasipolynomial-list-refuter
$R$ for $\MAJ_{n,\eps}$ against all query algorithms using $t(n) = o(1/\eps^2)$ queries with probability gap bounded by $1-2\delta$.
\end{theorem}
\begin{proofsketch}
The refuter is as follows: 
On input $1^n$, set $k = 1/\eps^2$, $q = 2^{\lceil \log_2 n \rceil}$ and run the $k$-wise independent generator from \cref{thm:kwiseindepgen}
for all of the $2^{O(k \log n)} = 2^{\polylog(n)}$ seeds for each of 
$p = p_+ = \left\lceil q \cdot (1/2 + 2\eps) \right\rceil$ and
$p = p_- = \left\lfloor q \cdot (1/2 - 2\eps) \right\rfloor$. 
Output all the results which satisfy the at least $(1/2+\eps)n$ ones or at most $(1/2-\eps)n$ ones promise. 

Consider the uniform probability distribution on the outputs of the refuter $\D_1$. 
This distribution is statistically close to the uniform distribution $\D_2$ on all the outputs
of the generator, without filtering those that do not satisfy the promise, since there are 
very few that do not satisfy the promise 
(a fraction of around $\Probsub{X \sim \text{Bin}(n, p_+/q)}{X < (1/2+\eps)n}$).
In turn, distribution $\D_2$ is indistinguishable by algorithms making $t(n)$ queries
from the corresponding distribution $\D_3$ in which either all $n$ bits are sampled from
independent Bernoullis with probability $p_+/q$ or all $n$ bits are sampled from Bernoullis with
probability $p_-/q$, each of the two options with probability $1/2$. 
And finally, distribution $\D_3$ is statistically close to distribution $\D_4$, which is distribution 
$\D_3$ conditioned on the output satisfying the promise. We conclude that query algorithms using $t(n)$
queries can not distinguish between distributions $\D_1$ and $\D_4$ except with some small probability
(exponentially small in $n$, in fact). \todowrite{Verify this claim.}

Distribution $\D_4$ is a symmetric distribution on $X^+_\eps \cup X^-_\eps$, 
and with similar arguments as in \cref{thm:querylb} 
we can prove that query algorithms with error probability $2\delta$ with inputs 
over distribution $\D_4$ require $\Omega(1/\eps^2)$ queries. Therefore, algorithms
with probability gap bounded by $1-2\delta$ must fail at least on a fraction $\delta$ of
inputs from $\D_4$. Therefore, they must also fail at a constant fraction of inputs from $\D_1$
and in particular they will fail for at least one of the elements outputted by $R(1^n)$, for $n$ big enough.

\end{proofsketch}

\todowrite{Write conclusion relating it to observations of the previous section.}

\subsection{Explicit Obstructions Do Not Work For Sufficiently }