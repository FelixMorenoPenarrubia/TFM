\todowritehp{Finish writing Chapter 3}

In this chapter, we study constructive separations in the query algorithm setting.
In particular, we study the problem $\MAJ$ of distinguishing determining whether
a binary string $x \in \{0, 1\}^n$ has at least $(\frac{1}{2}+\eps)n$ ones or
at most $(\frac{1}{2}-\eps)n$ ones, under the promise that it is one of the two
cases. 

We will prove the $\Omega(1/\eps^2)$ lower bound in the number of queries
for $\MAJ$, show a randomized constructive separation against algorithms
using fewer queries, and prove and discuss \cref{thm:csrefuterqa}, the result
from \cite{ConstructiveSeparations} showing that breakthrough lower
bounds follow from sufficiently constructive separations of $\MAJ$.  


\section{Lower Bounds Against Query Algorithms for $\MAJ$}

The obvious algorithm for $\MAJ$ consists in randomly sampling values and answering 
according to the majority of the values we have obtained. According to well-studied
bounds on the binomial distribution, $\Theta(1/\eps^2)$ samples
are necessary and sufficient to make the probability of error smaller than any 
fixed constant following this strategy.

In this section, we will prove the very intuitive result that this is essentially
the best that can be done, that is, that there is no query algorithm making $o(1/\eps^2)$
queries that gives the right answer with small probability of error. This was 
proved in \cite{Canetti95} in the more general context of \emph{samplers}: algorithms
that compute the average of a real-valued function $f \colon \{0, 1\}^n \to [0, 1]$
by sampling some of the inputs. 

The result that we will prove here is stronger than the one stated in \cite{Canetti95}:
we will prove not just that such query algorithms require $\Omega(1/\eps^2)$ in the 
worst case in order to have a high success probability \emph{for all} inputs; instead,
we will prove that $\Omega(1/\eps^2)$ queries are required for \emph{random} inputs,
where the probability of success is over the randomness of both the input and the algorithm.
This will allow us to obtain a constructive separation.
%However, the proof argument 
%we use is similar to the one used in \cite{Canetti95}.

\subsection{Proof of Lower Bounds for Random Inputs}

Fix $n$ and $\eps$, and let
$X^+_\eps = \{x \in \{0, 1\}^n | w_1(x) \geq (\frac{1}{2}+\eps )n \}$
and $X^-_\eps = \{x \in \{0, 1\}^n | w_1(x) \leq (\frac{1}{2}-\eps )n \}$.
\todowrite{define w1, etc in introduction}

For $\calD$ a probability distribution on $X^+_\eps \cup X^-_\eps$, denote
by $\calD^+$ the probability distribution conditioned on $x \in X^+_\eps$,
and by $\calD^-$ the probability distribution conditioned on $x \in X^-_\eps$.
We say that a probability distribution $\calD$ on $X^+_\eps \cup X^-_\eps$ is
symmetric if for every $y \in \{0, 1\}^*$ and every pair of subsets $S, S' \subset [n]$
with $|S| = |S'| = y$, 
$\Probsub{x \samplefrom \calD^+}{x[S] = y} = \Probsub{x \samplefrom \calD^+}{x[S'] = y}$ and
$\Probsub{x \samplefrom \calD^-}{x[S] = y} = \Probsub{x \samplefrom \calD^-}{x[S'] = y}$.
That is, the probability $\Probsub{x \samplefrom \calD^+}{x[S] = y}$ does not depend
on the set $S$: denote by $F_{\calD}^+(y)$ the probability
$\Probsub{x \samplefrom \D}{x[S] = y | x \in X^+_{\eps}}$ and by $F_{\calD}^-(y)$ the probability
$\Probsub{x \samplefrom \D}{x[S] = y | x \in X^-_{\eps}}$. 

\begin{lemma}
\label{lem:symmetricproperties}
Let $\calD$ be a symmetric distribution on $X^+_\eps \cup X^-_\eps$.

\begin{enumerate}
    \item The probabilities $F_{\calD}^+(y)$ and $F_{\calD}^-(y)$ only depend on the number of ones and zeros
    of $y$: $F_{\calD}^+(y) = f_{\D}^+(w_0(y), w_1(y)), F_{\D}^-(y) = f_{\D}^-(w_0(y), w_1(y))$.
    \item $f^+(a, b) = f^-(b, a)$ for all $a, b \geq 0$. 
    \item If $a \leq b$, then $f^-_\D(a, b) \leq f^+_\D(a, b)$.
\end{enumerate}
\end{lemma}
\begin{proof}
    TODO. \todowrite{prove symmetric distribution lemma}
\end{proof}

\begin{lemma}
\label{lem:symmetricbound}
Let $\calD$ be a symmetric distribution on $X^+_\eps \cup X^-_\eps$,
 and let $\A$ be a probabilistic query algorithm
that always does $t$ queries and solves $\MAJ$ with error probability $\delta$ 
on inputs sampled from $\calD$. 
Then:
$$
\sum_{i=0}^{\lceil t/2 \rceil-1} \binom{t}{i} f^+_{\D}(i, t-i) \leq \delta.
$$
\end{lemma}
\begin{proof}
Denote by $x$ the input, and let $Y = y_1 \ldots y_t$ denote the random variable corresponding
to the values of the $t$ sampled points. Note that $Y$ depends on $x$ and also on the randomness
over the execution of $Y_r$ be the random variable corresponding to the sampled points where
the randomness of $\A$ has been fixed to be $r$. $Y_r$ only depends on the choice of $x$.

Let $P^+ = \Prob{\A(x) = 0 | x \in X^+_\eps}$ the probability that the algorithm errs
conditioned on $x$ having a majority of ones, and similarly let 
$P^- = \Prob{\A(x) = 1 | x \in X^-_\eps}$ be the probability that the algorithm errs conditioned
on $x$ having a majority of zeros.

We have:

$$
P^+ = \sum_{y\in \{0, 1\}^t} \Prob{Y = y | x \in X^+_\eps} \Prob{\A(x) = 0 | Y = y, x \in X^+_\eps}.
$$

Now note the following:

\begin{itemize}
    \item Once $y$ is fixed, the result of $\A$ does not depend on $x$, but only on $y$ and $r$.
    Hence $\Probsub{x,r}{A(x) = 0 | Y = y, x \in X^+_\eps} = \Probsub{r}{\A(x) = 0 | Y = y}$.
    We denote this probability by $G_\A(y)$.
    \item Since $\D$ is symmetric, the event $Y = y$ does not depend on the
    randomness $r$ of the algorithm and in fact $\Prob{Y = y | x \in X^+_\eps} = F^+_{\D}(y)$.
    This is because, fixing some randomness $r$ in the algorithm, the event that the sequence
    of $t$ sample points is equal to $y$ is equivalent to the event that a subsequence of
    $x[S]$ of $x$ of size $t$ is equal to a particular permutation of $\sigma(y)$ of $y$. 
    By \cref{lem:symmetricproperties}, this probability does not depend on $S$ or $\sigma$.
\end{itemize}

Thus, 

$$
P^+ = \sum_{y \in \{0, 1\}^t} F^+_\D(y) \cdot G_\A(y).
$$

Similarly:

$$
P^- = \sum_{y \in \{0, 1\}^t} F^-_\D(y) \cdot (1-G_\A(y)).
$$

Adding them and applying \cref{lem:symmetricproperties}, we get the desired result:
\begin{align*}
\delta & \geq \frac{1}{2}\left(P^+ + P^-\right) \\
        & = \frac{1}{2}\sum_{y \in \{0, 1\}^t} F^+_\D(y) \cdot G_\A(y) + F^-_\D(y) \cdot (1-G_\A(y)) \\ 
        & \geq \frac{1}{2}\sum_{y \in \{0, 1\}^t} \min \{F^+_\D(y), F^-_\D(y)\} \\
        & = \frac{1}{2}\sum_{i=0}^{t} \binom{t}{i} \min \{f^+_\D(i, t-i), f^+_\D(t-i, i)\} \\
        & \geq \sum_{i=0}^{\lceil t/2 \rceil-1} \binom{t}{i} f^+_\D(i, t-i).
\end{align*}

\end{proof}

\begin{theorem}
\label{thm:querylb}
Let $\A$ be a query algorithm that solves the $\MAJ$ problem on inputs uniformly drawn from 
$S_n = \{x \in \{0, 1\}^n | w_1(x) \in \{\lfloor(1/2-\eps)n\rfloor, \lceil(1/2+\eps)n\rceil\}\}$ 
with failure probability $\delta(n)$ using always at most $t(n)$ queries, with $t(n) \leq \frac{1}{4}\sqrt{n}$. 
There exists an absolute constant $C$ so that: 
$$
t(n) \geq \frac{1}{4\eps^2} \log \left(\frac{C}{\delta}\right)
$$

for all sufficiently large $n$.
\end{theorem}

\begin{proof}

First, we can assume that the algorithm always samples exactly $t(n)$ distinct points. Otherwise,
we can create an algorithm $A'$ which samples additional points before answerting
until exactly $t(n)$ points have been sampled.

For each $n$, the uniform distribution $U_{S_n}$ on $S_n$ is a symmetric distribution. 
Therefore, by \cref{lem:symmetricbound}, we have:

$$
\sum_{i=0}^{\lceil t/2 \rceil-1} \binom{t}{i} f^+_{U_{S_n}}(i, t-i) \leq \delta(n).
$$

Let $k = \lceil(1/2+\eps)n\rceil$. We can compute $f^+_{U_{S_n}}$:


\begin{equation}
\label{eq:fcalc}
f^+_{U_{S_n}}(w, t-w) = \frac{\binom{n-t}{k-w}}{\binom{n}{k}} = \frac{(n-t)!k!(n-k)!}{n!(k-w)!(((n-k)-(t-w))!}
    = \frac{(k)_w}{(n)_w} \frac{(n-k)_{t-w}}{(n-w)_{t-w}}
\end{equation}

Where $(n)_h = \frac{n!}{h!}$ is the falling factorial. We use the following inequalities 
(TODO references \todonit{references for factorial inequalities})
    for the falling
factorial:

$$
n^h e^{-\frac{h^2}{2(n-h)}} \leq (n)_h \leq n^h e^{-\frac{h(h-1)}{2n}}
$$

So we have:

$$
\frac{(k)_w}{(n)_w} \geq \frac{k^we^{-\frac{w^2}{2(n-w)}}}{n^we^{-\frac{w(w-1)}{2(n-w)}}} = \left(\frac{k}{n}\right)^w e^{-w\left(\frac{w}{2(k-w)}-\frac{w-1}{2n}\right)}
$$

Recall that $w \leq t \leq \frac{1}{4}\sqrt{n}$, and that $k > \frac{1}{2}n$. Using this, we can see that for sufficiently large $n$, we have that $e^{-w\left(\frac{w}{2(k-w)}-\frac{w-1}{2n}\right)} \geq 1/\sqrt{2}$, so:

$$
\frac{(k)_w}{(n)_w} \geq \frac{1}{\sqrt{2}}  \left(\frac{k}{n}\right)^w
$$

Similarly, for sufficiently large $n$ we have:

$$
\frac{(n-k)_{t-w}}{(n-w)_{t-w}} \geq \frac{1}{\sqrt{2}} \left(\frac{n-k}{n-w} \right)^{t-w} > \frac{1}{\sqrt{2}} \left(\frac{n-k}{n} \right)^{t-w}
$$

Substituting in \eqref{eq:fcalc}, we have:

$$
f^+_{U_{S_n}}(w, t-w)  \geq \frac{1}{2} \left(\frac{k}{n}\right)^w \left(\frac{n-k}{n} \right)^{t-w}
$$
    
So that:

\begin{align*}
\delta & \geq \sum_{i=0}^{\lceil t/2 \rceil-1} \binom{t}{i} f^+_{U_{S_n}}(i, t-i) \\
       & \geq \sum_{i=0}^{\lceil t/2 \rceil-1} \frac{1}{2} \binom{t}{i} \left(\frac{k}{n}\right)^i \left(\frac{n-k}{n} \right)^{t-i} \\
       & = \frac{1}{2} \Probsub{X \sim \text{Bin}(n, k/n)}{X < \left\lceil \frac{t}{2} \right\rceil}
\end{align*}

By standard bounds on the tail of the binomial distribution,
(TODO reference and details \todonit{reference for tails of binomial distribution})
we have that there exists a constant $C$ so that 
$\Probsub{X \sim \text{Bin}(n, k/n)}{X < \left\lceil \frac{t}{2} \right\rceil} \geq C e^{-4\eps^2t}$.
Rearranging, we get our desired result. 

\end{proof}

\begin{corollary}
Let $\eps(n)$ be a function satisfying $\eps(n) = \omega(n^{-1/4})$ and $\eps(n) = o(1)$. Then, all query
algorithms $\A$ solving $\MAJ$ with bounded failure probability for all inputs do at least $\Omega(1/\eps(n)^2)$
queries. 
\end{corollary}
\begin{proof}
TODO \todowrite{prove corollary, and state it more precisely too}
\end{proof}

\subsection{Randomized Constructive Separation}

Explain how to make a randomized constructive separation from above (amplify error probability).



\section{Sufficiently Constructive Separations Imply Lower Bounds}

State and prove possible strengthenings of the result of \cite{ConstructiveSeparations}. 

\thmcsrefuterqa*

\section{Constructive Separations Against Weaker Query Algorithms}

In \cref{thm:csrefuterqa}, there are two important quantitative assumptions
that must hold in order to obtain a lower bound:

\begin{enumerate}
    \item We must have a refuter against all $t$-juntas with $t = O(\log \eps^-1 / \eps^{1+\delta})$ for
    some $\delta > 0$.
    \item The function $\eps(n)$ must go to zero sufficiently fast: $\eps(n) = 1/(\log n)^{\omega(1)}$.
\end{enumerate}

We can wonder whether those conditions are \emph{tight}, in the sense that if we relax them slightly 
then we can have a constructive separation. 

\subsection{Explicit Obstructions for $t = o(1/\eps)$}

Explain that for query algorithms making $o(1/\eps)$ queries, explicit-obstructions refuters exist.

\todoidea{what happens with O(1/eps)?}

\subsection{Derandomization of Query Algorithms}

Explain $t$-wise independent distributions and how they help when $\eps = O(1)$. 
